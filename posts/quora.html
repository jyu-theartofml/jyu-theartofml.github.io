<!doctype html>
<html lang="en">

<head>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The Art of Machine Learning</title>
  <meta name="description" content="Machine Learning, Deep Learning, Jenny Yu ">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Regression for taxi data">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://yinniyu.github.io/posts/taxi">
  <meta property="og:site_name" content="The Art of Machine Learning">
  <meta property="og:image" content="http://chalk.nielsenramon.com/assets/documentation/sample-image.jpg">

 
  <link rel="apple-touch-icon" href="/assets/apple-touch-icon.png">
  <link href="http://chalk.nielsenramon.com/feed.xml" type="application/rss+xml" rel="alternate" title="Chalk Last 10 blog posts" />

    <link rel="stylesheet" href="/assets/light.css">

 </head>


<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav appear">
  <a href="/" class="header-logo" title="The Art of Machine Learning">The Art of Machine Learning</a>
  <ul class="header-links">

     <li>
        <a href="https://github.com/yinniyu" rel="noreferrer noopener" target="_blank" title="GitHub">
          <span class="icon icon-github"></span>
        </a>
      </li>
    
    
      <li>
        <a href="https://linkedin.com/in/jenny-yu-b495243" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <span class="icon icon-linkedin"></span>
        </a>
      </li>   
  </ul>
</nav>

        
        <article class="article appear">
          <header class="article-header">
            <h1>Designing a classifier to detect duplicate questions </h1>
            <p>For this Kaggle competition that took place 6 months ago, the goal is to compile a model to identify if a pair of questioins is asking the same thing. 
              Quora provided 400K+ question pairs for the training set, and the final test data set has 2,345,796 question pairs (that's alot of data!). 
              Many Kagglers have tried techniques such as Xgboost and feature extraction such as TF-IDF, ratio of matching words, and weighted word2vec. 
              I wanted to learn how recurrent neural network works, and explore its potential in solving this problem. The final 
              ranking for this submission was top 37% on Kaggle's private LB.</p>
            
            <div class="article-list-footer">
              <span class="article-list-date">
                Nov 2, 2017
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
              10 minute read
              </span>
              
              <span class="article-list-divider">-</span>
              <div class="article-list-tags">
                
                <span class="article-list-divider">Python, LSTM, Deep Learning, Kaggle </span>
              </div>
            </div>
          </header>

          <div class="article-content">

<p>The training data for this project was very straightforward - one column for each question, and another 
  column that states whether the pair was a duplicate (1 is duplicate, 0 is no). There're two things that make
  this dataset different than the vanilla binary classification problem. First, these are sequential data which means
  the context of a particular word depends on the preceeding word(s). Second, the sentences have varied lengths so
  the input data do not have fixed size. Recurrent Neural Net is designed to tackle problems with these characteristics, so
  let's delve into more details about this model.
  for such Two codes are shown in this repository - 
  the first one is the preprocessing step to build the embedding matrix, and the second code is for model training and tuning. 
  Log loss is used to evaluate the performance of the model. 
              </p>

            

  

<p>Now time for some contour plot, which requires both <i> ggmap</i> and <i> ggplot</i> libraries. <i>ggmap</i> 
    is used to retrieve the map of a locale using <code class="highlighter-rouge"> get_map()</code></p>
<figure class="highlight"><pre><code class="language-R" data-lang="R">
p <- get_map("New York City",zoom=14,maptype='toner-lite',source="stamen")
#select columns with the given pattern with dplyr.
data_pickup<-data %>% select (starts_with("pickup_l"))
data_dropoff<-data %>% select (starts_with("dropoff_l"))
colnames(data_pickup)<-c('lon', 'lat')
colnames(data_dropoff)<-c('lon', 'lat')
combo_data<-rbind(data.frame(data_pickup, group="pickup"), data.frame(data_dropoff, group="dropoff"))
#transparency of the plot (alpha) can be set to level of the sample points (there's 10 levels because there's 10 bins)
ggmap(p)+ stat_density_2d(data=combo_data, geom ='polygon', bins=10, aes(x=lon, y=lat, fill=group, alpha=..level..))+
  scale_fill_manual(values=c('pickup'='orchid4', 'dropoff'='darkorange1'))+
  scale_alpha(guide = FALSE)
</code></pre></figure> 
 
<figure>
   <img src="images/contour.png" alt="contour map" >
   <figcaption>Fig 2. Contour map of two groups of data.
   </figcaption>
   </figure>

<h2 id="feature engineering">Feature Engineering for Regression</h2>

<p>Most of the time, machine learning workflows can benefit from feature engineering. Here, feature engineering is defined as adding or 
transforming relevant information/variables 
to the existing dataset in order to improve the model's learning and its performance. In some situations, feature 
engineering requires extensive domain knowledge, for this taxi dataset it was pretty straight forward. The new features obtained 
from the given features are the time stamp element (i.e, month, date, weekday, hour) and the <a href='https://www.r-bloggers.com/great-circle-distance-calculations-in-r/'>
haversine distance</a> in km. The target variable, <i> trip duration</i> was log transformed since it displayed a highly skewered
distribution. I generally prefer to have normal-ish distribution when working with continunous variable, because it helps some models to  
learn better. In fact, you might consider normalization/scaling to be a form of feature engineering because the raw data are transformed to
optimize distance-based algorithms or gradient descend parameter updates.</p>

<p>Two codes are shown in this repository - 
  the first one is the preprocessing step to build the embedding matrix, and the second code is for model training and tuning. 
  Log loss is used to evaluate the performance of the model. </p>

<figure class="highlight"><pre><code class="language-R" data-lang="R">
library(GGally)
#using only 10K data points so it doesn't take too long to plot
sample_data<-datasets[1:10000,]
ggpairs(sample_data[, c("passenger_count","pickup_hour","pickup_day", "pickup_month", "pickup_weekday","distance_h", "log_duration")])
</code></pre></figure>

<figure>
   <img src="images/scatter_plot_matrix.png" alt="correlation plot" >
   <figcaption>Fig 3. Scatter plots of variables below the diagonal. The diagonal plots correspond to the distribution of the given variable. For
   example, for the plot in the 6th row and 3rd column, the x-axis is the pickup calendar day and the y-axis is the haversine distance.
   </figcaption>
   </figure>


<h2 id="randomeforest">Caret and the Forest</h2>

<p>After the data are preprocessed, I used the <i> Caret</i>  library to perform grid search with cross validation (CV). I chose 
Random Forest (RF) regressor because it is known to work well with nonlinear data, it selects random 
subset of features at each node split, and it reduces variance (aka: overfitting) of the model by having independent weak learner trees. </p>
<p>Since the trees in RF are independent of each other, it is easy to setup parallel computing.
This is helpful for running RF on large dataset with ~1M samples. On the <i> Caret</i>  documentation, the author only mentioned 
<i> doMC</i>  for parallel computing but that only works for Macs. I have a windows 10 laptop with 4 cores (8 processors), and found that
the <i>doParallel</i>  package worked well with the CV workflow.
<figure class="highlight"><pre><code class="language-R" data-lang="R">
library(randomForest)
library(caret)
library(parallel)
library(doParallel)
library(dplyr)
datasets<- datasets%>% mutate_if(is.integer, as.numeric)
trainIndex <- createDataPartition(datasets$log_duration, p = .7, 
                                  list = FALSE, 
                                  times = 1)
data_train<-datasets[trainIndex,]
data_val<-datasets[-trainIndex,]
#### grid must include mtry for RF model ####
rfgrid<- expand.grid(mtry=c(6,13))
set.seed(20)
cv_control <- trainControl(
  method = "cv",
  number = 3)
  
###### NOTE: CV and grid functions has to come before cluster computing setup  ##########

cluster <- makeCluster(detectCores()-3)# leave at least 1 node for operating system 
registerDoParallel(cluster)

##train function will invoke dummy variables for formula input!
##but randomForest library itself doesn't do that with formula input
rfFit <- train(log_duration ~ ., data = data_train, 
                 method = "rf", ntree=100,
                 trControl = cv_control, 
                 tuneGrid=rfgrid,
                 verbose = FALSE, importance=TRUE)

stopCluster(cluster)
registerDoSEQ()
## to see error of the model
rfFit$results
</code></pre></figure>

<p> Running the model fit on only 200K samples (to save time), the default metric RMSE was found to be about 0.44. There's still 
room for improvement, especially by running the whole training set. As you can see, it's easy to perform cross validated parameter
tuning on a Random Forest regressor through <i>Caret</i>, which comes with other widely used models such as SVM, Logistic Regression, 
and GLM regression, etc (total of 238).
        


          
            <div id="disqus_thread" class="article-comments"></div>
            <script src="https://chalk-1.disqus.com/embed.js" async defer></script>
            <noscript>Please enable JavaScript to view the comments.</noscript>
          
        </article>
        <footer class="footer appear">
  <p>
    Chalk is a high quality, completely customizable, performant and 100% free
    blog template for Jekyll built by
    <a href="/about" title="About me">Nielsen Ramon</a>. Download it <a href="https://github.com/nielsenramon/chalk" rel="noreferrer noopener" target="_blank" title="Download Chalk">here</a>.
  </p>
</footer>

      </div>
    </div>
  </main>
  
  <script>
    window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
    ga('create','UA-28631876-6','auto');ga('send','pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async></script>


<script src="/assets/vendor.js"></script>



  <script src="/assets/webfonts.js"></script>




  <script src="/assets/scrollappear.js"></script>



<script src="/assets/application.js"></script>


</body>
</html>
