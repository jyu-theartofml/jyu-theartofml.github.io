<!doctype html>
<html lang="en">

<head>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The Art of Machine Learning</title>
  <meta name="description" content="Machine Learning, Deep Learning, Jenny Yu ">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Automatic multiple object tracking">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://jyu-theartofml.github.io/posts/object_tracking.html">
  <meta property="og:site_name" content="The Art of Machine Learning">
  <meta property="og:image" content="images/object_tracking/posture_snap.png">
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" href="/assets/apple-touch-icon.png">
  <link href="http://chalk.nielsenramon.com/feed.xml" type="application/rss+xml" rel="alternate" title="Chalk Last 10 blog posts" />

  <link rel="stylesheet" href="/assets/light.css">

</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav appear">
  <a href="/" class="header-logo" title="The Art of Machine Learning">The Art of Machine Learning</a>
  <ul class="header-links">

    <li>
        <a href="https://jyu-theartofml.github.io/about">About Me
        </a>
      </li>

    <li>
        <a href="https://jyu-theartofml.github.io/toolbox">My Toolbox</a>
     </li>
     <li>
        <a href="https://jyu-theartofml.github.io/ai_art">AiArt
        </a>
      </li>

    <li>
        <a href="https://github.com/jyu-theartofml" rel="noreferrer noopener" target="_blank" title="GitHub">
          <span class="icon icon-github"></span>
        </a>
      </li>


      <li>
        <a href="https://linkedin.com/in/jenny-yu-b495243" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <span class="icon icon-linkedin"></span>
        </a>
      </li>
  </ul>
</nav>


        <article class="article appear">
          <header class="article-header">
            <h1>Automatic multiple object tracking with OpenCV</h1>
             <figure>
              <center><img src='images/object_tracking/posture_snap.png' alt="object detection" width="100%" height="100%"></center>
               </figure>
            <p>Image analysis and object detection have always been something I'm interested in, my first exposure was
              writing a Matlab program to automatically detect fluorescent microarray spots and extract the pixel intensity values.
              A while back I took the Udemy course <b>Deep Learning and Computer Vision A-Z </b>by Hadelin de Ponteves and Kirill Eremenko. One of the projects
            was object detection using the Deep learning model Single Shot MultiBox Detector (SSD). I wanted to expand on that project and come up with a straightforward way to
          track the movement of detected objects across multiple classes in video feeds. This is the basis for applications like
          traffic monitoring, estimating velocity, and security monitoring.</p>

        <p>After browsing the computer vision blog <u><a href='https://www.pyimagesearch.com/'>Pyimagesearch</a></u>, I came across some example tutorials on object tracking. However,
          the examples I came across are either for tracking single-class object or not recording the object class. So I modified the codes to include multi-class object trackings based
          on the output of the SSD detection. For this post, I will share the custom functions that I came up with to integrate multiple class and multiple label tracking using OpenCV,
        as well as posture estimation.</p>

            <div class="article-list-footer">
              <span class="article-list-date">
                June 10th, 2020
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
              10 minute read
              </span>

              <span class="article-list-divider">-</span>
              <div class="article-list-tags">

                <span class="article-list-divider">OpenCV, Object Detection, PyTorch</span>
              </div>
            </div>
          </header>

          <div class="article-content">


<h2>Disentangle the matrix</h2>

<p>The SSD model showed significant improvement in detection speed due to the elimination of bounding box proposal and resampling of pixels, while achieving high accuracy[1]. I won't go into details on how SSD model works and the training mechanism,
  if you're interested check out the in-depth <a href='https://mc.ai/real-time-object-detection-part-1-understanding-ssd/'>article </a> written by Aman Dalmia. What is most
  relevant to this post is the output matrix of SSD. By default, when the model makes an inference on an image, it returns 8,732 bounding boxes for each class to accommodate for the range of
  scale and aspect ratios. Then it tidys up the prediction using non-maximum suppression to remove overlapping boxes which exceeds the IoU threshold of 0.45, keeping only the top 200 boxes total.
  This means that the immediate output array from an image is (N, i, j, b), where N is the batch axis (1 for one image/frame), i is the class, j is the number of detection, and b is the axis containing the confidence score
  and bounding box coordinates.
</p>
  <p>The approach used to track the objects can be broken down into a few steps.</p>
  <ol>
    <li>Within a frame, calculate the centroid coordinates for each bounding box, assign the proper ID and class label.</li>
    <li>Update the dictionary list of the centroids detected.</li>
    <li>Check the updated list to see if current centroids match centroids in the previous frame, corresponding to the same object.
      If yes, draw out the tracking.</li>
  </ol>

  <p>Keeping that in mind, the following function returns the bounding box matrix "detections". The function takes one frame, and adds the first axis for batching, and runs a
  single forward pass through the pretrained network. The scale variable is needed to later transform the array back to the original frame size.</p>
  <figure class="highlight"><pre><code class="language-Python" data-lang="Python">
    def detect(frame, net, transform):
       """
       frame: individual frame
       net: pretrained object detector model
       transform: function to transform image of the frame
       """
       height, width = frame.shape[:2]
       frame_t = transform(frame)[0]
       x = torch.from_numpy(frame_t).permute(2, 0, 1)
       x = x.unsqueeze(0)
       # We feed the neural network ssd with the image to get prediction output.
       with torch.no_grad():
           y = net(x)
       # the dimension is (N,i,j,b), i is the class, j is the number of detection, b axis is confidence and bounding box coordinates
       detections = y.data
       scale = torch.Tensor([width, height, width, height])
 </code></pre></figure>

 <p>Next, the array is looped through by each class, and boxes with confidence score
   greater than or equal to 0.6 are selected. And the bounding boxes are drawn using <code class='highlighter-rouge'>cv2.rectangle()</code>.
 Note that many of the OpenCV utility drawing functions directly overwrites the pixels in that image, so the original image is altered.</p>

   <figure class="highlight"><pre><code class="language-Python" data-lang="Python">
      for i in range(detections.size(1)):
          j = 0
          rects = []
          # for each class, filter for bbox with confidence score >=0.6.
          while detections[0, i, j, 0] >= 0.6:
            # SSD returns the points at the upper left and the lower right of the bbox.
            pt = (detections[0, i, j, 1:] * scale).numpy()
            cv2.rectangle(
               frame,
               (int(pt[0]), int(pt[1])),
               (int(pt[2]), int(pt[3])),
               (255, 0, 0),
               2,
           )

            # save the bounding box coordinates
            # (startX, startY, endX, endY)
            rects.append((int(pt[0]), int(pt[1]), int(pt[2]), int(pt[3])))
            j += 1
            objects = ct.update(rects)
   </code></pre></figure>
<p>The bbox coordinates are saved in the object instance <code class='highlighter-rouge'>ct = CentroidTracker()</code>,
  which was a class function provided by Adrian Rosebrock @ Pyimagesearch to calculate the centroid and assign its ID label[2].
  The underlying assumption is that the distance between a given object centroid for frames t and t-1 will be smaller
 than its distances to other object centroids. Having this dictionary allows us to loop over the labeled objects within each class and draw the centroids as shown below. </p>
<figure class="highlight"><pre><code class="language-Python" data-lang="Python">
       for (objectID, centroid) in objects.items():
               # draw the centroid of the object on the output frame and put text on the frame
               text = labelmap[i - 1] + "_" + str(objectID)
               cv2.putText(
                   frame,
                   text,
                   (centroid[0] - 10, centroid[1] - 10),
                   cv2.FONT_HERSHEY_SIMPLEX,
                   2,
                   (0, 255, 0),
                   2,
                   cv2.LINE_AA,
               )
               cv2.circle(frame, (centroid[0], centroid[1]), 10, (0, 255, 0), -1)
</code></pre></figure>

<h2>Follow the centroid</h2>

<p>Centroid tracking is a convenient tool that allows for frame-to-frame tracking of movement, like the path of a bicyclist.
I modified the above <code class='highlighter-rouge'>detect</code> function to allow for movement tracking by identifying the same label ID from the same class in the previous frame.
And using the <code class='highlighter-rouge'>deque</code> object from the <i>collections</i> package to update the list of centroids for each frame. Here's the complete function
that loops through each class, and updates the list of centroids, then draws the movement in lines color-coded by class.</p>

<figure class="highlight"><pre><code class="language-Python" data-lang="Python">
  def tracking(frame, net, transform, f_idx, track_pts):
    """
    frame: individual frame
    net: pretrained object detector mdoel
    transform: function to transform image of the frame
    f_idx: frame count
    track_pts: deque collection of centroids
    """
    height, width = frame.shape[:2]
    frame_t = transform(frame)[0]
    x = torch.from_numpy(frame_t).permute(2, 0, 1)
    x = x.unsqueeze(0)
    with torch.no_grad():
        # We feed the neural network ssd with the image to get prediction output.
        y = net(x)
    # the dimension is (N,i,j,b), i is the class, j is the number of detection (200), b axis is confidence and bounding box coordinates
    detections = y.data
    scale = torch.Tensor([width, height, width, height])

    # keep track of centroid points for this individual frame
    centroid_pts = {f_idx: {}}
    color_ls = [
        (79, 111, 243),
        (129, 199, 190),
        (86, 160, 82),
        (181, 190, 183),
        (49, 90, 93),
        (142, 132, 21),
        (69, 116, 8),
        (9, 68, 193),
        (126, 152, 167),
        (103, 95, 90),
        (98, 117, 184),
        (186, 112, 111),
        (234, 54, 27),
        (90, 218, 83),
        (134, 185, 119),
        (174, 1, 141),
        (2, 222, 238),
        (219, 86, 39),
        (155, 151, 186),
        (192, 221, 232),
        (174, 149, 85),
    ]

    for i in range(detections.size(1)):
        j = 0
        rects = []
        # for each class, filter for bbox with confidence score >=0.6.
        while detections[0, i, j, 0] >= 0.6:
            # SSD returns the points at the upper left and the lower right of the bbox
            pt = (detections[0, i, j, 1:] * scale).numpy()
            cv2.rectangle(
                frame,
                (int(pt[0]), int(pt[1])),
                (int(pt[2]), int(pt[3])),
                color_ls[i],
                3,
            )
            # save the bounding box coordinates (startX, startY, endX, endY)
            rects.append((int(pt[0]), int(pt[1]), int(pt[2]), int(pt[3])))
            j += 1
            objects = ct.update(rects)

            # loop over the tracked centroids
            centroid_pts[f_idx][i] = {}
            for (objectID, centroid) in objects.items():
                # add centroid pt to list for each class
                w = {objectID: centroid}
                centroid_pts[f_idx][i].update(w)

                #  draw the centroid of the object on the output frame
                cv2.circle(frame, (centroid[0], centroid[1]), 15, color_ls[i], -1)
                text = labelmap[i - 1] + "_" + str(objectID)
                cv2.putText(
                    frame,
                    text,
                    (centroid[0] - 10, centroid[1] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    2,
                    (0, 255, 0),
                    2,
                    cv2.LINE_AA,
                )

    track_pts.appendleft(centroid_pts)  # update the deque list
    # loop through track_pts dictionary and plot the line of the object movement
    for i in range(len(track_pts) - 1):
        frame_idx = list(track1[i].keys())[0]
        current = track1[i][frame_idx]
        for class_idx, v in current.items():
            for label_id, coord in v.items():
                try:
                    y = track1[i][frame_idx][class_idx][label_id]
                    y_t = track1[i + 1][frame_idx - 1][class_idx][label_id]
                    # draw the line
                    thickness = int(np.sqrt(64 / float(i + 1)) * 2.5)
                    cv2.line(
                        frame, tuple(y_t), tuple(y), color_ls[class_idx], thickness
                    )
                except KeyError:
                    pass

    return frame, track_pts

</code></pre></figure>
<p>This function has the additional argument `f_idx` and `track_pts` because it needs to keep track of the frame sequence, and it needs a pre-initialized dictionary object.
Here, the dictionary `track_pts` can have specified maxlength to keep the line drawn at a certain max length. The critical part is the loops towards the end, where it
checks to see if the current centroid (y) shares matching label ID and class with centroids in the previous frame (y_t). An output sample clip is shown below.</p>


<video width="600" controls>
  <source src="images/object_tracking/street_day_output.mp4" type="video/mp4">
</video>
<p>There're some misclassified objects in between frames, and only the foreground runners were detected. As a result, some of the movement tracking were off due to the mislabeling.
Like many object detection model, SSD did not perform as well when there's substantial background noises blending in with the image and the foreground scenery. Next, I tried out the algorithm
on a second clip that captures street traffic at night.</p>
<video width="600" controls>
  <source src="images/object_tracking/street_nite_tracking.mp4" type="video/mp4">
</video>
<p>This dim lit video yielded worse detection performance, mislabeling some cars as person. Whenever there are other objects
positioned directly behind the pedestrian, the model struggles to make the right classification, and at times no classfication was made for the pedestrian.
In terms of the detection, it is well known that object detection models experience significant decrease in performance when they make inference on dark images.
One way to mitigate that is to preprocess the video feeds to retrieve the details, using pretrained models like <a href='https://www.youtube.com/watch?v=qWKUFK7MWvg&feature=youtu.be'>See in the Dark (SID)</a>
to transform low-light images to well-lit RGB outputs[3].</p>
<p>For continuous tracking, we can try to reduce the "noise" by increasing the default value of <code class='highlighter-rouge'>maxDisappeared=50</code> in the CentroidTracker class function, where
 <code class='highlighter-rouge'>maxDisappeared</code> is the number of maximum consecutive frames a given object is allowed to be marked as "disappeared"
 until its tracking stops. We can also adjust `y` and `y_t` to skip a frame (e.g i+2,frame_idx - 2) so it's more robust to the occasional misclassification in between frames.</p>

<h2>2D posture detection</h2>

<p>There has been a wealth of research and open source programs to detect human poses, with the most promising library <b>OpenPose</b>
released by the Perceptual Computing Lab @ Carnegie Mellon University, and won the COCO keypoints challenge in 2016. It uses the first 10
layers of the VGGnet as input for downstream CNN network to predict confidence maps of body key points (shoulders, hands, legs, etc), and
the affinity vectors between parts to estimate the 2D keypoint coordinates for the human subjects[4].</p>
<p>I utilitized the body estimator model from the <a href='https://github.com/Hzzone/pytorch-openpose'>pytorch-openpose</a> repo. For rendering the skeleton,
  the model returns a total of 18 key points from an array of candidate points. After extracting those key points, I applied trigonometric
functions to pick out the runners in the video frame based on angles of the arms and legs.</p>

<video width="600" controls>
  <source src="images/object_tracking/posture_output.mp4" type="video/mp4">
</video>

<p>The result performance was sensitive to "loud" background, and the overlapping and proximity of the subjects in the frame.
  For some odd reason, the pedestrians walking past the trash cans were misclassified as "Runner" in a few frames.
  I managed to fine-tune the angle thresholds to reduce the false positives of "Runner" detection. To
run one frame without a GPU it took about 59 seconds, with a Colab Tesla T4 notebook it was cut down to ~2.5 seconds.</p>

<p>For detection tasks that preserve privacy, you can substitue the output video with a base frame (a frame that only shows the plain background)
  and only show the bounding boxes and the trackings.
  I have written another code to accomodate that, and the example output can be downloaded <a href='https://github.com/jyu-theartofml/jyu-theartofml.github.io/blob/master/posts/images/object_tracking/street_day_baseframe.mp4?raw=true'>here</a>.
  You can view <b>the complete codes for the material presented in this post in my
  github <a href='https://github.com/jyu-theartofml/object_tracking'>repo</a></b>. And feel free to leave comments here.</p>

<br></br>


<h3> REFERENCES </h3>
<p>[1]Liu, W., Anguelov, D., Erhan, D., et al. <i>SSD: Single Shot MultiBox Detector</i>, arXiv preprint arXiv:1512.02325, 2015.</p>
<p>[2]Rosebrock, A., <b>Simple object tracking with OpenCV</b>, https://www.pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/</p>
<p>[3]Z,I., Safer YOLO, in the Dark (I), https://medium.com/@turboergouzhi/safer-yolo-in-the-dark-i-98ddaa7db3ad</p>
<p>[4]Z Cao, G Hidalgo, T Simon, SE Wei, Y Sheikh, <i>OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields</i>,arXiv preprint arXiv:1812.08008</p>
<div id="disqus_thread" class="article-comments"></div>
<script src="https://chalk-1.disqus.com/embed.js" async defer></script>
<noscript>Please enable JavaScript to view the comments.</noscript>

</article>
<footer class="footer appear">
  <p>
    Chalk is a high quality, completely customizable, performant and 100% free
    blog template for Jekyll built by
    <a href="/about" title="About me">Nielsen Ramon</a>. Download it <a href="https://github.com/nielsenramon/chalk" rel="noreferrer noopener" target="_blank" title="Download Chalk">here</a>.
  </p>
</footer>

      </div>
    </div>
  </main>

  <script>
    window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
    ga('create','UA-28631876-6','auto');ga('send','pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async></script>


<script src="/assets/vendor.js"></script>



  <script src="/assets/webfonts.js"></script>




  <script src="/assets/scrollappear.js"></script>



<script src="/assets/application.js"></script>


</body>
</html>
