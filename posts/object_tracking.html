<!doctype html>
<html lang="en">

<head>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The Art of Machine Learning</title>
  <meta name="description" content="Machine Learning, Deep Learning, Jenny Yu ">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Automatic multiple object tracking">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://jyu-theartofml.github.io/posts/object_tracking.html">
  <meta property="og:site_name" content="The Art of Machine Learning">
  <meta property="og:image" content=" ">
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" href="/assets/apple-touch-icon.png">
  <link href="http://chalk.nielsenramon.com/feed.xml" type="application/rss+xml" rel="alternate" title="Chalk Last 10 blog posts" />

  <link rel="stylesheet" href="/assets/light.css">

</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav appear">
  <a href="/" class="header-logo" title="The Art of Machine Learning">The Art of Machine Learning</a>
  <ul class="header-links">

    <li>
        <a href="https://jyu-theartofml.github.io/about">About Me
        </a>
      </li>

    <li>
        <a href="https://jyu-theartofml.github.io/toolbox">My Toolbox</a>
     </li>
     <li>
        <a href="https://jyu-theartofml.github.io/ai_art">AiArt
        </a>
      </li>

    <li>
        <a href="https://github.com/jyu-theartofml" rel="noreferrer noopener" target="_blank" title="GitHub">
          <span class="icon icon-github"></span>
        </a>
      </li>


      <li>
        <a href="https://linkedin.com/in/jenny-yu-b495243" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <span class="icon icon-linkedin"></span>
        </a>
      </li>
  </ul>
</nav>


        <article class="article appear">
          <header class="article-header">
            <h1>Automatic multiple object tracking with OpenCV</h1>
             <figure>
              <center><img src='images/object_tracking/epic_horse_snap.png' alt="object detection" width="100%" height="100%"></center>
               </figure>
            <p>A while back I took the Udemy course <b>Deep Learning and Computer Vision A-Z </b>by Hadelin de Ponteves and Kirill Eremenko. One of the projects
            was object detection using Deep learning model such as Single Shot MultiBox Detector (SSD). The SSD
             model showed significant improvement in detection speed due to the elimination of bounding box proposal and resampling of pixels, while achieving high accuracy[1].
           After being expose to object detection models and some basic computer vision tools via OpenCV, I wanted to expand on that project and come up with a simple way to
          track the movement of detected objects across multiple classes. This is handy for applications like estimating distance, velocity, and traffic density.</p>

        <p>After doing some research, I found articles on the awesome <a href='https://www.pyimagesearch.com/'>Pyimagesearch</a> blog that offered example tutorials on object detection. However,
          the examples I came across are either for tracking single-class object or not recording the object class. So I tweaked the codes to include multi-class object trackings based
          on the output of the SSD detection. For this post, I will share the custom functions that I came up with to integrate multiple class tracking using OpenCV.

            <div class="article-list-footer">
              <span class="article-list-date">
                June, 2020
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
              10 minute read
              </span>

              <span class="article-list-divider">-</span>
              <div class="article-list-tags">

                <span class="article-list-divider">OpenCV, Object Detection, PyTorch</span>
              </div>
            </div>
          </header>

          <div class="article-content">


<h2>Disentangle the matrix</h2>

<p>The code base I have are written for SSD300 (input image size 300x300) and PyTroch 1.5. I won't go into details on how SSD model works and the training mechanism,
  if you're interested there's an in-depth <a href='https://mc.ai/real-time-object-detection-part-1-understanding-ssd/'>article </a> written by Aman Dalmia. What is most
  relevant to this post is the output matrix of SSD. By default, when the model makes an inference on an image, it returns 8732 bounding boxes for each class to accommodate for the range of
  scale and aspect ratios. Then it tidys up the prediction using non-maximum suppression to remove overlapping boxes which exceeds the IoU threshold of 0.45, keeping only the top 200 boxes total.
  This means that the immediate output array from an image is (N, i, j, b), where N is the batch axis (1 for one image/frame), i is the class, j is the number of detection, and b is the axis containing the confidence score
  and bounding box coordinates.
</p>

  <p>Keeping that in mind, the following function returns the bounding box matrix "detections". The function takes one frame, and adds the first axis for batching, and runs a
  single forward pass through the pretrained network. The scale variable is needed to later transform the array back to the original frame size.</p>
  <figure class="highlight"><pre><code class="language-Python" data-lang="Python">
    def detect(frame, net, transform):
       """
       frame: individual frame
       net: pretrained object detector model
       transform: function to transform image of the frame
       """
       height, width = frame.shape[:2]
       frame_t = transform(frame)[0]  # We apply the transformation to our frame.
       # We convert the frame into a torch tensor.
       x = torch.from_numpy(frame_t).permute(2, 0, 1)
       x = x.unsqueeze(0)  # We add a fake dimension corresponding to the batch.
       # We feed the neural network ssd with the image and we get the output y.
       with torch.no_grad():
           y = net(x)
       # We create the detections tensor contained in the output y,
       # the dimension is (N,i,j,b), i is the class, j is the number of detecttion, b axis is confidence and bounding box coordinates
       detections = y.data
       scale = torch.Tensor([width, height, width, height])
 </code></pre></figure>

 <p>Next, the array is looped through by each class, and boxes with confidence score
   greater than or equal to 0.6 are selected. And the bounding boxes are drawn using <code class='highlighter-rouge'>cv2.rectangle()</code>.
 Note that many of the CV utility drawing functions directly overwrites the pixels in that image, so the original image is altered.</p>

   <figure class="highlight"><pre><code class="language-Python" data-lang="Python">
      for i in range(detections.size(1)):
          # We initialize the loop variable j that will correspond to the occurrences of the class.
          j = 0
          rects = []
          # keep track of occurrences j of the class i
          while detections[0, i, j, 0] >= 0.6:
            # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.
            pt = (detections[0, i, j, 1:] * scale).numpy()
            cv2.rectangle(
               frame,
               (int(pt[0]), int(pt[1])),
               (int(pt[2]), int(pt[3])),
               (255, 0, 0),
               2,
           )

            # save the bounding box coordinates
            # (startX, startY, endX, endY)
            rects.append((int(pt[0]), int(pt[1]), int(pt[2]), int(pt[3])))
            j += 1
            objects = ct.update(rects)
   </code></pre></figure>
<p>The bbox coordinates are saved in the object instance <code class='highlighter-rouge'>ct = CentroidTracker()</code>,
  which was a class function provided by Adrian Rosebrock @ Pyimagesearch to calculate and group the centroids by its ID. Having this
dictionary allows us to loop over the labeled objects in each class and draw the centroids as shown below. </p>
<figure class="highlight"><pre><code class="language-Python" data-lang="Python">
       for (objectID, centroid) in objects.items():
               # draw the centroid of the object on the output frame and put text on the frame
               text = labelmap[i - 1] + "_" + str(objectID)
               cv2.putText(
                   frame,
                   text,
                   (centroid[0] - 10, centroid[1] - 10),
                   cv2.FONT_HERSHEY_SIMPLEX,
                   2,
                   (0, 255, 0),
                   2,
                   cv2.LINE_AA,
               )
               cv2.circle(frame, (centroid[0], centroid[1]), 10, (0, 255, 0), -1)
</code></pre></figure>
<p>Centroid tracking</p> 
<h2>The types of API </h2>
<p> The AWS document labyrinth made it difficult to piece things together in a cohesive fashion. So I did some
  digging around and found some useful literature with clear description of the SM system. In essense, the API
  has three tiers: Amazon SageMaker SDK (high-level), AWS SDK for Python (Boto3), and awscli & Docker (low-level)[2].</p>

<p> The high-level Amazon SageMaker SDK is the most common, especially for beginners. It is hosted in a Jupyter Notebook IDE, and the user
  trains the model by specifying the instance types, and number of distributed workers. The SageMaker SDK will then
  automate the training jobs and the subsequent deployment to an endpoint. However, SageMaker SDK does not work with event-driven Lambda functions, and that's
  where Boto3 comes in [3]. The Boto3 API requires that the source code be uploaded to S3, and specifying the relevant configurations for SageMaker to run. It has more low-level features
since Boto3 is the SDK for all of the AWS ecosystem. On the other hand, the awscli & docker API offers more ways to customize codes and system configuration. It's ideal for situations where
the model or algorithm you want to use is not supported by SageMaker.</p>
 <p> This blog post will focus only on the Amazon SageMaker SDK, because it's more user friendly and intuitive. By using the San Francisco crime dataset, I will
    walk through a straight forward tutorial on training a Xgboost classifier with hyperparameter search. I chose this after realizing that most SageMaker tutorials for xgboost are only for <i>binary</i> classifier and not multiclass, and doesn't
    offer more in depth details on the hyperparameter tuning API. </p>

<h2>Training with SageMaker</h2>

<p>To get started, head to the Amazon SageMaker Dashboard. On the left menu, select <i>Notebook instances</i>. To create a new notebook, click on
  <i>Create notebook instance</i> (see Fig.2). At a minimum, it expects an instance type of ml.t2.medium.</p>
  <figure>
   <center><img src='images/sagemaker/notebook_instance_sm.png' alt="sagemaker new notebook instance" width="80%" height="80%"></center>
   <figcaption><i>Fig.2. Setting up a notebook instance. Under "Additional configuration" the user can specify the volume size of the SM notebook.</i>
    </figure>

<p>To train models in SM notebook, the datasets need to be stored in S3, and its path need to be established using <code class='highlighter-rouge'>sagemaker.s3_input</code>.
    To instantiate the xgboost model, you need to call the docker image for SM xgboost as shown in the code below. The output_path is the S3 bucket that stores the trained model as .tar.gz file.</p>

    <figure class="highlight"><pre><code class="language-Python" data-lang="Python">
   sess = sagemaker.Session()
   container = get_image_uri(region, 'xgboost', '0.90-1')
   role = get_execution_role()
   bucket = 'kaggle.sf.crime'

   xgb = sagemaker.estimator.Estimator(container,
                                       role,
                                       train_instance_count=1,
                                       train_instance_type='ml.m4.xlarge',
                                       output_path='s3://{}/output/'.format(bucket),
                                       sagemaker_session=sess)

  # for multiclass target the user needs to specify num_class, otherwise it won't work, for some reason, can't change learning rate

  xgb.set_hyperparameters(
  objective='multi:softmax',
  eval_metric='merror',
  num_round=100,
  colsample_bytree=1,
  gamma=1.2,
  seed=2,
  num_class=len(concat_df['crime'].unique().tolist())
  )
  objective_metric_name = 'validation:merror'
   </code></pre></figure>

  <p>At this point, the model object is ready for training by calling <code class='highlighter-rouge'>xgb.fit({'train':'s3://my-bucket/training', 'validation':'s3://my-bucket/validation})</code>.
    An alternative is to perform hyperparameter tuning by setting up the parameter ranges as show below. Here, max_jobs refers to the number of sampling points from the grid range, and <code class='highlighter-rouge'>HyperparameterTuningJobAnalytics </code> returns
    a table showing the status of the training jobs and the objective metric used to evaluated the model on
   the validation set (under <i>FinalObjectiveValue</i>).</p>

  <figure class="highlight"><pre><code class="language-Python" data-lang="Python">
  hyperparameter_ranges = {
       'subsample': ContinuousParameter(0.5, 1),
       'max_depth': IntegerParameter(3, 10)
   }

   gridsearch= HyperparameterTuner(
       xgb,
       objective_metric_name,
       hyperparameter_ranges,
       objective_type='Minimize',
       max_jobs=5,
       max_parallel_jobs=10,
       early_stopping_type='Auto',
       strategy='Random')

   gridsearch.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)

   sagemaker.HyperparameterTuningJobAnalytics(gridsearch.latest_tuning_job.job_name).dataframe()

 </code></pre></figure>

<p>As show in Fig.3., the SM console shows the comprehensive activity status while the notebook is running. On the left panel,
 you can go to <i> Hyperparameter tuning jobs</i> under <i>Training</i> to view the progress of the training. Not only can
 you see the training and validation errors, but it also shows the CPU and memory utilization metrics.</p>
 <figure>
  <center><img src='images/sagemaker/SM_job_status.png' alt="sagemaker status" width="85%" height="85%"> </center>
  <figcaption><i>Fig.3. SageMaker provides overall status update.</i>
   </figure>


<h2>Endpoint deployment</h2>
<p>When the model is ready for deployment after parameter tuning, the best parameter can be automatically attached to the model Estimator.
 The user needs to specify the number of instance and type of instance for deploying the model. The endpoint is an object identifiable
   by an Amazon Resource Name(ARN) and defaults to private setting (keep reading to see how to share this resouce with IAM role permission).
  </p>

  <figure class="highlight"><pre><code class="language-Python" data-lang="Python">
  # Attach to an existing hyperparameter tuning job, which consists of multiple training job tasks
  xgb_tuning_job_name = gridsearch.latest_tuning_job.job_name
  xgb_tuner = HyperparameterTuner.attach(xgb_tuning_job_name)

  # Get the best XGBoost training job name from thejob
  xgb_best_job = xgb_tuner.best_training_job()
  xgb_tuned = sagemaker.estimator.Estimator.attach(xgb_best_job)

  # to deploy and then invoke the endpoint to make inference
  xgb_tuned.deploy(initial_instance_count=1, instance_type='ml.t2.medium')

  endpt_predictor=sagemaker.predictor.RealTimePredictor(endpoint=xgb_best_job)
  endpt_predictor.content_type='text/csv'
  endpt_predictor.serializer=csv_serializer
  endpt_predictor.deserializer= None

  select_cols=cols[1:]
  arr_val=validation[select_cols][-10:].values
  pred_result=endpt_predictor.predict(arr_val).decode("utf-8").split(',')
 </code></pre></figure>

<p>Keep in mind that <b>charges will incur</b> while the endpoint is being hosted and the notebook instance
   is running. You can delete the endpoint from the console by going to <i>Endpoints</i> under <i>Inference</i>,
   and you can stop or delete the notebook by going to the <i> Notebook instances</i>. </p>

<h2>Invoking the endpoint</h2>
<p>If you are working in a team and would like to share access to the model endpoint for future development and/or updating,
 it's best practice to assign permission based on a policy and IAM role. To invoke the endpoint, I
 followed a similar approach as this <a href='https://towardsdatascience.com/sharing-your-sagemaker-model-eaa6c5d9ecb5'> article</a> by Zak Jost.
 I first had to set up a custom policy for a specific role ("Version" of policy should be "2012-10-17"). Then
  I created a third party account (child account) with a new user. The child account id is required in order
  for the parent account to grant the permission to the child. For this to work,
the child account must have a user that will assume the role with its attached policy, for some reason using the root does *not* work.

</p>

<figure class="highlight"><pre><code class="language-Python" data-lang="Python">
# after calling the client with boto3, invoke endpoint.
endpoint_name='sagemaker-xgboost-200204-1841-002-fc901d44'

# payload_file is StringIO class that writes out strings
response = client.invoke_endpoint(
    EndpointName= endpoint_name,
    Body= payload_file.getvalue(),
    ContentType = 'text/csv')

str_result = response['Body'].read().decode()
</code></pre></figure>

<p>For more details on compiling the codes, I've uploaded my notebooks at this <a href='https://github.com/jyu-theartofml/kaggle_SFCrime/tree/master/sagemaker'> link</a>.
The notebook "xgboost_sm_train" should be run as a SM notebook instance, and "xgboost_sm_inference" can be run on a local computer with the
 right awscli <a href='https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html'> configuration</a>. You
 will also need the parent account ID (the account that grants the policy) for assuming the role.</p>

 <h2>Beyond the basics</h2>
 <p>The material in this post covers the fundamentals to get started with AWS SageMaker, but there's more to it!
  If you want to retrain the model with new data on a schedule, and automatically push the updated model to the endpoint,
 then you need tools like Step Function or Lambda. Lambda function can be managed by AWS CloudWatch to set up resources and schedules.
 For future project, I'd like to turn the current endpoint I have into a public API using Lambda and API Gateway.</p>


<h3> REFERENCES </h3>
<p>[1]Liu, W., Anguelov, D., Erhan, D., et al. <i>SSD: Single Shot MultiBox Detector</i>,arXiv preprint arXiv:1512.02325, 2015.</p>

<div id="disqus_thread" class="article-comments"></div>
<script src="https://chalk-1.disqus.com/embed.js" async defer></script>
<noscript>Please enable JavaScript to view the comments.</noscript>

</article>
<footer class="footer appear">
  <p>
    Chalk is a high quality, completely customizable, performant and 100% free
    blog template for Jekyll built by
    <a href="/about" title="About me">Nielsen Ramon</a>. Download it <a href="https://github.com/nielsenramon/chalk" rel="noreferrer noopener" target="_blank" title="Download Chalk">here</a>.
  </p>
</footer>

      </div>
    </div>
  </main>

  <script>
    window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
    ga('create','UA-28631876-6','auto');ga('send','pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async></script>


<script src="/assets/vendor.js"></script>



  <script src="/assets/webfonts.js"></script>




  <script src="/assets/scrollappear.js"></script>



<script src="/assets/application.js"></script>


</body>
</html>
