<!doctype html>
<html lang="en">

<head>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The Art of Machine Learning</title>
  <meta name="description" content="Machine Learning, Deep Learning, Jenny Yu ">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Automatic multiple object tracking">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://jyu-theartofml.github.io/posts/object_tracking.html">
  <meta property="og:site_name" content="The Art of Machine Learning">
  <meta property="og:image" content="images/object_tracking/epic_horse_snap.png">
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" href="/assets/apple-touch-icon.png">
  <link href="http://chalk.nielsenramon.com/feed.xml" type="application/rss+xml" rel="alternate" title="Chalk Last 10 blog posts" />

  <link rel="stylesheet" href="/assets/light.css">

</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav appear">
  <a href="/" class="header-logo" title="The Art of Machine Learning">The Art of Machine Learning</a>
  <ul class="header-links">

    <li>
        <a href="https://jyu-theartofml.github.io/about">About Me
        </a>
      </li>

    <li>
        <a href="https://jyu-theartofml.github.io/toolbox">My Toolbox</a>
     </li>
     <li>
        <a href="https://jyu-theartofml.github.io/ai_art">AiArt
        </a>
      </li>

    <li>
        <a href="https://github.com/jyu-theartofml" rel="noreferrer noopener" target="_blank" title="GitHub">
          <span class="icon icon-github"></span>
        </a>
      </li>


      <li>
        <a href="https://linkedin.com/in/jenny-yu-b495243" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <span class="icon icon-linkedin"></span>
        </a>
      </li>
  </ul>
</nav>


        <article class="article appear">
          <header class="article-header">
            <h1>Automatic multiple object tracking with OpenCV</h1>
             <figure>
              <center><img src='images/object_tracking/epic_horse_snap.png' alt="object detection" width="100%" height="100%"></center>
               </figure>
            <p>A while back I took the Udemy course <b>Deep Learning and Computer Vision A-Z </b>by Hadelin de Ponteves and Kirill Eremenko. One of the projects
            was object detection using the Deep learning model Single Shot MultiBox Detector (SSD). The SSD
             model showed significant improvement in detection speed due to the elimination of bounding box proposal and resampling of pixels, while achieving high accuracy[1].
             I wanted to expand on that project and come up with a straightforward way to
          track the movement of detected objects across multiple classes in video feeds. This is the basis for applications like estimating distance, velocity, and traffic density.</p>

        <p>After doing some research, I found articles on the awesome <u><a href='https://www.pyimagesearch.com/'>Pyimagesearch</a></u> blog that offered example tutorials on object detection. However,
          the examples I came across are either for tracking single-class object or not recording the object class. So I tweaked the codes to include multi-class object trackings based
          on the output of the SSD detection. For this post, I will share the custom functions that I came up with to integrate multiple class tracking using OpenCV.</p>

            <div class="article-list-footer">
              <span class="article-list-date">
                June 10th, 2020
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
              10 minute read
              </span>

              <span class="article-list-divider">-</span>
              <div class="article-list-tags">

                <span class="article-list-divider">OpenCV, Object Detection, PyTorch</span>
              </div>
            </div>
          </header>

          <div class="article-content">


<h2>Disentangle the matrix</h2>

<p>The code base I have were written for SSD300 (input image size 300x300) and PyTroch 1.5l, the packages <a href='https://pypi.org/project/opencv-python/'>OpenCV</a>
  and <a href='https://imageio.readthedocs.io/en/stable/installation.html'>Imageio</a> are also required. I won't go into details on how SSD model works and the training mechanism,
  if you're interested there's an in-depth <a href='https://mc.ai/real-time-object-detection-part-1-understanding-ssd/'>article </a> written by Aman Dalmia. What is most
  relevant to this post is the output matrix of SSD. By default, when the model makes an inference on an image, it returns 8732 bounding boxes for each class to accommodate for the range of
  scale and aspect ratios. Then it tidys up the prediction using non-maximum suppression to remove overlapping boxes which exceeds the IoU threshold of 0.45, keeping only the top 200 boxes total.
  This means that the immediate output array from an image is (N, i, j, b), where N is the batch axis (1 for one image/frame), i is the class, j is the number of detection, and b is the axis containing the confidence score
  and bounding box coordinates.
</p>

  <p>Keeping that in mind, the following function returns the bounding box matrix "detections". The function takes one frame, and adds the first axis for batching, and runs a
  single forward pass through the pretrained network. The scale variable is needed to later transform the array back to the original frame size.</p>
  <figure class="highlight"><pre><code class="language-Python" data-lang="Python">
    def detect(frame, net, transform):
       """
       frame: individual frame
       net: pretrained object detector model
       transform: function to transform image of the frame
       """
       height, width = frame.shape[:2]
       frame_t = transform(frame)[0]  # We apply the transformation to our frame.
       # We convert the frame into a torch tensor.
       x = torch.from_numpy(frame_t).permute(2, 0, 1)
       x = x.unsqueeze(0)  # We add a fake dimension corresponding to the batch.
       # We feed the neural network ssd with the image and we get the output y.
       with torch.no_grad():
           y = net(x)
       # We create the detections tensor contained in the output y,
       # the dimension is (N,i,j,b), i is the class, j is the number of detecttion, b axis is confidence and bounding box coordinates
       detections = y.data
       scale = torch.Tensor([width, height, width, height])
 </code></pre></figure>

 <p>Next, the array is looped through by each class, and boxes with confidence score
   greater than or equal to 0.6 are selected. And the bounding boxes are drawn using <code class='highlighter-rouge'>cv2.rectangle()</code>.
 Note that many of the CV utility drawing functions directly overwrites the pixels in that image, so the original image is altered.</p>

   <figure class="highlight"><pre><code class="language-Python" data-lang="Python">
      for i in range(detections.size(1)):
          # We initialize the loop variable j that will correspond to the occurrences of the class.
          j = 0
          rects = []
          # keep track of occurrences j of the class i
          while detections[0, i, j, 0] >= 0.6:
            # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.
            pt = (detections[0, i, j, 1:] * scale).numpy()
            cv2.rectangle(
               frame,
               (int(pt[0]), int(pt[1])),
               (int(pt[2]), int(pt[3])),
               (255, 0, 0),
               2,
           )

            # save the bounding box coordinates
            # (startX, startY, endX, endY)
            rects.append((int(pt[0]), int(pt[1]), int(pt[2]), int(pt[3])))
            j += 1
            objects = ct.update(rects)
   </code></pre></figure>
<p>The bbox coordinates are saved in the object instance <code class='highlighter-rouge'>ct = CentroidTracker()</code>,
  which was a class function provided by Adrian Rosebrock @ Pyimagesearch to calculate and group the centroids by its ID[2]. Having this
dictionary allows us to loop over the labeled objects within each class and draw the centroids as shown below. </p>
<figure class="highlight"><pre><code class="language-Python" data-lang="Python">
       for (objectID, centroid) in objects.items():
               # draw the centroid of the object on the output frame and put text on the frame
               text = labelmap[i - 1] + "_" + str(objectID)
               cv2.putText(
                   frame,
                   text,
                   (centroid[0] - 10, centroid[1] - 10),
                   cv2.FONT_HERSHEY_SIMPLEX,
                   2,
                   (0, 255, 0),
                   2,
                   cv2.LINE_AA,
               )
               cv2.circle(frame, (centroid[0], centroid[1]), 10, (0, 255, 0), -1)
</code></pre></figure>

<h2>Draw out the movement</h2>

<p>Centroid tracking is a convenient tool that allows for frame-to-frame tracking of movement, like the path of a bicyclist.
I modified the above <code class='highlighter-rouge'>detect</code> function to allow for movement tracking by identifying the same label ID from the same class in the previous frame.
And using the <code class='highlighter-rouge'>deque</code> function from the <i>collections</i> package to update the list of centroids for each frame. Here's the complete function
that includes additional looping to draw the movement in lines.</p>

<figure class="highlight"><pre><code class="language-Python" data-lang="Python">
  def tracking(frame, net, transform, f_idx, track_pts):
    """
    frame: individual frame
    net: pretrained object detector mdoel
    transform: function to transform image of the frame
    f_idx: frame count
    track_pts: deque collection of centroids
    """
    height, width = frame.shape[:2]  # We get the height and the width of the frame.
    frame_t = transform(frame)[0]  # We apply the transformation to our frame.
    # We convert the frame into a torch tensor.
    x = torch.from_numpy(frame_t).permute(2, 0, 1)
    x = x.unsqueeze(0)  # We add a fake dimension corresponding to the batch.
    with torch.no_grad():
        # We feed the neural network ssd with the image and we get the output y.
        y = net(x)
    # the dimension is (N,i,j,b), i is the class, j is the number of detecttion, b axis is confidence and bounding box coordinates
    detections = y.data
    scale = torch.Tensor([width, height, width, height])

    # keep track of centroid points for this individual frame
    centroid_pts = {f_idx: {}}
    color_ls = [
        (79, 111, 243),
        (129, 199, 190),
        (86, 160, 82),
        (181, 190, 183),
        (49, 90, 93),
        (142, 132, 21),
        (69, 116, 8),
        (9, 68, 193),
        (126, 152, 167),
        (103, 95, 90),
        (98, 117, 184),
        (186, 112, 111),
        (234, 54, 27),
        (90, 218, 83),
        (134, 185, 119),
        (174, 1, 141),
        (2, 222, 238),
        (219, 86, 39),
        (155, 151, 186),
        (192, 221, 232),
        (174, 149, 85),
    ]

    for i in range(detections.size(1)):  # For every class:
        # We initialize the loop variable j that will correspond to the occurrences of the class.
        j = 0
        rects = []
        # We take into account all the occurrences j of the class i that have a matching score larger than 0.6.
        while detections[0, i, j, 0] >= 0.6:
            # We get the coordinates of the points at the upper left and the lower right of the detector rectangle.
            pt = (detections[0, i, j, 1:] * scale).numpy()
            cv2.rectangle(
                frame,
                (int(pt[0]), int(pt[1])),
                (int(pt[2]), int(pt[3])),
                color_ls[i],
                3,
            )
            # save the bounding box coordinates (startX, startY, endX, endY)
            rects.append((int(pt[0]), int(pt[1]), int(pt[2]), int(pt[3])))
            j += 1
            objects = ct.update(rects)

            # loop over the tracked centroids
            centroid_pts[f_idx][i] = {}
            for (objectID, centroid) in objects.items():
                # add centroid pt to deque list for each class
                w = {objectID: centroid}
                centroid_pts[f_idx][i].update(w)

                #  draw the centroid of the object on the output frame
                cv2.circle(frame, (centroid[0], centroid[1]), 15, color_ls[i], -1)
                text = labelmap[i - 1] + "_" + str(objectID)
                cv2.putText(
                    frame,
                    text,
                    (centroid[0] - 10, centroid[1] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    2,
                    (0, 255, 0),
                    2,
                    cv2.LINE_AA,
                )

    track_pts.appendleft(centroid_pts)  # update the deque list
    # loop through track_pts dictionary and plot the line of the object movement
    for i in range(len(track_pts) - 1):
        frame_idx = list(track1[i].keys())[0]
        current = track1[i][frame_idx]
        for class_idx, v in current.items():
            for label_id, coord in v.items():
                try:
                    y = track1[i][frame_idx][class_idx][label_id]
                    y_t = track1[i + 1][frame_idx - 1][class_idx][label_id]
                    # draw the line
                    thickness = int(np.sqrt(64 / float(i + 1)) * 2.5)
                    cv2.line(
                        frame, tuple(y_t), tuple(y), color_ls[class_idx], thickness
                    )
                except KeyError:
                    pass

    return frame, track_pts

</code></pre></figure>
<p>This function has the additional argument `f_idx` and `track_pts` because it needs to keep track of the frame sequence, and it needs a pre-initialized dictionary object.
Here, the dictionary `track_pts` can have specified maxlength to keep the line drawn at a certain max length. The critical part is the loops towards the end, where it
checks to see if the current centroid (y) shares matching label ID and class with centroids in the previous frame (y_t). An output sample clip is shown below.</p>


<video width="600" controls>
  <source src="images/object_tracking/horse_tracking.mp4" type="video/mp4">
</video>
<p>There's some misclassified objects that were labeled as cows, and only the foreground horses were detected. As a result, some of the movement tracking were off due to the mislabeling.
Like many object detection model, SSD did not perform as well when there's substantial background noises blending in with the image and the foreground scenery. Next, I tried out the algorithm
on a second clip that captures street traffic at night.</p>
<video width="600" controls>
  <source src="images/object_tracking/street_tracking.mp4" type="video/mp4">
</video>
<p>The SSD model yielded worse detection performance, mislabeling some cars as person. Whenever there are other objects
positioned directly behind the pedestrian, the model struggles to make the right classification, and at times no classfication was made for the pedestrian.
One thing to try for reducing the "noise" is to modify the <code class='highlighter-rouge'>ct = CentroidTracker()</code> method. Instead of
using the default last frame(t-1) to associate old centroids with new centroids corresponding to the same object,
we can try to skip a frame (t-2), holding the same underlying assumption that
the distance between a given object centroid for frames t and t-2 will be smaller
than its distances to other object centroids. </p>

<p>If you are interested, <b>the complete codes for the material presented in this post can be found in my
  github <a href='https://github.com/jyu-theartofml/object_tracking'>repo</a></b>. And feel free to leave comments here.</p>

<br></br>


<h3> REFERENCES </h3>
<p>[1]Liu, W., Anguelov, D., Erhan, D., et al. <i>SSD: Single Shot MultiBox Detector</i>, arXiv preprint arXiv:1512.02325, 2015.</p>
<p>[2]Simple object tracking with OpenCV, https://www.pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/</p>
<div id="disqus_thread" class="article-comments"></div>
<script src="https://chalk-1.disqus.com/embed.js" async defer></script>
<noscript>Please enable JavaScript to view the comments.</noscript>

</article>
<footer class="footer appear">
  <p>
    Chalk is a high quality, completely customizable, performant and 100% free
    blog template for Jekyll built by
    <a href="/about" title="About me">Nielsen Ramon</a>. Download it <a href="https://github.com/nielsenramon/chalk" rel="noreferrer noopener" target="_blank" title="Download Chalk">here</a>.
  </p>
</footer>

      </div>
    </div>
  </main>

  <script>
    window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
    ga('create','UA-28631876-6','auto');ga('send','pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async></script>


<script src="/assets/vendor.js"></script>



  <script src="/assets/webfonts.js"></script>




  <script src="/assets/scrollappear.js"></script>



<script src="/assets/application.js"></script>


</body>
</html>
