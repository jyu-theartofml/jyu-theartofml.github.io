<!doctype html>
<html lang="en">

<head>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The Art of Machine Learning</title>
  <meta name="description" content="Machine Learning, Deep Learning, Jenny Yu, denver ">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Data imputation">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://yinniyu.github.io/posts/imputation.html">
  <meta property="og:site_name" content="The Art of Machine Learning">
  <meta property="og:image" content=>

 
  <link rel="apple-touch-icon" href="/assets/apple-touch-icon.png">
  <link href="http://chalk.nielsenramon.com/feed.xml" type="application/rss+xml" rel="alternate" title="Chalk Last 10 blog posts" />

    <link rel="stylesheet" href="/assets/light.css">

  
</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav appear">
  <a href="/" class="header-logo" title="The Art of Machine Learning">The Art of Machine Learning</a>
  <ul class="header-links">

    <li>
        <a href="https://yinniyu.github.io/about">About Me
        </a>
      </li>
    
    <li>
        <a href="https://yinniyu.github.io/toolbox">My Toolbox</a>
      </li>
    
     <li>
        <a href="https://github.com/yinniyu" rel="noreferrer noopener" target="_blank" title="GitHub">
          <span class="icon icon-github"></span>
        </a>
      </li>
    
    
      <li>
        <a href="https://linkedin.com/in/jenny-yu-b495243" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <span class="icon icon-linkedin"></span>
        </a>
      </li>   
  </ul>
</nav>

        
        <article class="article appear">
          <header class="article-header">
            <h1>Imputing missing data</h1>
            <p>Missing data is a familiar nuisance when working with raw dataset. In this post, I will compare three widely used methods for 
            imputing (a.k.a,estimating) missing values. This is really for my own reference, because I often find myself asking the question "which imputation 
            algorithm would be most appropriate" while staring at those 'NA's in the dataframe. I tend to avoid simply imputing using
            the mean/median/mode value, because it doesn't take into account possible correlation to other variables and it introduces 
            bias to the data.</p>
            <p>The three imputation methods I will compare are: K Nearest Neighbor (KNN), Random Forest imputation, and Predictive Mean Matching (PMM). The
              first two methods work for both categorical and numerical values, and PMM works only for continuous numerical variable.
            
            <div class="article-list-footer">
              <span class="article-list-date">
                X X, 2018
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
              10 minute read
              </span>
              
              <span class="article-list-divider">-</span>
              <div class="article-list-tags">
                
                <span class="article-list-divider">R, imputation</span>
              </div>
            </div>
          </header>

          <div class="article-content">

<p>For the purpose of this post, I used the Titanic dataset, which contained a mix of categorical, ordinal, and numeric features.
  Here's what the data looks like: </p>
<div style="overflow-x:auto;">
<table >
  <colgroup span="11"></colgroup>
  <tr>
    <th>id</th>
    <th>vendor_id</th>
    <th>pickup_datetime</th>
    <th>dropoff_datetime</th>
    <th>passenger_count</th>
    <th>pickup_longitude</th>
    <th>pickup_latitude</th>
    <th>dropoff_longitude</th>
    <th>dropoff_latitude</th>
    <th>store_and_fwd_flag</th>
    <th>trip_duration (in seconds)</th>
    
  </tr>
  <tr>
    <td>id2875421</td>
    <td>2</td>
    <td>2016-03-14 17:24:55</td>
    <td>2016-03-14 17:32:30</td>
    <td>1</td>
    <td>-73.98215</td>
    <td>40.76794</td>
    <td>-73.96463</td>
    <td>40.76560</td>
    <td>N</td>
    <td>455</td>
  </tr>
  
  <tr>
    <td>id2377394</td>
    <td>1</td>
    <td>2016-06-12 00:43:35</td>
    <td>2016-06-12 00:54:38</td>
    <td>1</td>
    <td>-73.98042</td>
    <td>40.73856</td>
    <td>-73.99948</td>
    <td>40.73115</td>
    <td>N</td>
    <td>663</td>
    </tr>
  
  </table>
</div>
 <br>
 <p> I was interested to see the sensitivity of each imputation algorithm to percentage of missing values. For a given percentage,
   I randomly sample the row data for features PClass (Passenger Class) and Age, and set those values to NA for downstream imputation.
    This was done for 10%, 20%, 30%, and 40% missing values for both columns, so the data frame being imputed had missing numeric values as well
           as categorical value. In evaluating performance, I used (1-accuracy) to represent the error for Pclass imputation,
             and Mean Absolute Error (MAE) for the Age imputation. I chose MAE after seeing that the distribution of the Age feature was more or less 
            normal. For non-normal skewed distribution, it's recommended to use Root Mean Square Error (RMSE) instead.</p>
              

<h2 id="KNN">K Nearest Neighbor (KNN) </h2>

<p>Like its classifier counterpart, KNN imputes the missing value by finding the K nearest neighbors in feature space, then 
            assign the class by majority vote of its K nearest neigbors. For regression, the value is the weighted average of its 
            k nearest neighbors.</p>
<p>But how do we measure the distance between samples in feature space? For numeric variables, it's common to use euclidean distance (l2 norm). 
  However, imputing missing value often requires using information from categorical variables, so Gower's distance is used to accomodate 
  both numeric and categorical values in measuring <i>dissimilarity</i>. For the knn imputer in R library VIM, the Gower's distance is defined as :
  
  
  

<p>For categorical variable, d_k is 0 when the values between a pair of sample points are the same, otherwise it's 1. 
   On the other hand, when working with numeric data d_k is calculated as 1-[(x_i-x_j)/(max(x)-min(x)] [1]. </p>

<figure class="highlight"><pre><code class="language-R" data-lang="R">
library(GGally)
#using only 10K data points so it doesn't take too long to plot
sample_data<-datasets[1:10000,]
ggpairs(sample_data[, c("passenger_count","pickup_hour","pickup_day", "pickup_month", "pickup_weekday","distance_h", "log_duration")])
</code></pre></figure>

<figure>
   <img src="images/scatter_plot_matrix.png" alt="correlation plot" >
   <figcaption>Fig 3. Scatter plots of variables below the diagonal. The diagonal plots correspond to the distribution of the given variable. For
   example, for the plot in the 6th row and 3rd column, the x-axis is the pickup calendar day and the y-axis is the haversine distance.
   </figcaption>
   </figure>


<h2 id="randomeforest">Caret and the Forest</h2>

<p>After the data are preprocessed, I used the <i> Caret</i>  library to perform grid search with cross validation (CV). I chose 
<a href="https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html">Random Forest (RF) regressor </a> because it is known to work well with nonlinear data, it selects random 
subset of features at each node split, and it reduces variance (aka: overfitting) of the model by having independent weak learner trees. </p>
<p>Since the trees in RF are independent of each other, it is easy to setup parallel computing.
This is helpful for running RF on large dataset with ~1M samples. On the <i> Caret</i>  documentation, the author only mentioned 
<i> doMC</i>  for parallel computing but that only works for Macs. I have a windows 10 laptop with 4 cores (8 processors), and found that
the <i>doParallel</i>  package worked well with the CV workflow.
<figure class="highlight"><pre><code class="language-R" data-lang="R">
library(randomForest)
library(caret)
library(parallel)
library(doParallel)
library(dplyr)
datasets<- datasets%>% mutate_if(is.integer, as.numeric)
trainIndex <- createDataPartition(datasets$log_duration, p = .7, 
                                  list = FALSE, 
                                  times = 1)
data_train<-datasets[trainIndex,]
data_val<-datasets[-trainIndex,]
#### grid must include mtry for RF model ####
rfgrid<- expand.grid(mtry=c(6,13))
set.seed(20)
cv_control <- trainControl(
  method = "cv",
  number = 3)
  
###### NOTE: CV and grid functions has to come before cluster computing setup  ##########

cluster <- makeCluster(detectCores()-3)# leave at least 1 node for operating system 
registerDoParallel(cluster)

##train function will invoke dummy variables for formula input!
##but randomForest library itself doesn't do that with formula input
rfFit <- train(log_duration ~ ., data = data_train, 
                 method = "rf", ntree=100,
                 trControl = cv_control, 
                 tuneGrid=rfgrid,
                 verbose = FALSE, importance=TRUE)

stopCluster(cluster)
registerDoSEQ()
## to see error of the model
rfFit$results
</code></pre></figure>

<p> Running the model fit on only 400K samples (to save time), the average validation RMSE was around 0.4135. There's still 
room for improvement, especially by running the whole training set. As you can see, it's easy to perform cross validated parameter
tuning on a Random Forest regressor through <i>Caret</i>, which comes with other widely used models such as SVM, Logistic Regression, 
and GLM regression, etc (total of 238).
        


          
            <div id="disqus_thread" class="article-comments"></div>
            <script src="https://chalk-1.disqus.com/embed.js" async defer></script>
            <noscript>Please enable JavaScript to view the comments.</noscript>
          
        </article>
        <footer class="footer appear">
  <p>
    Chalk is a high quality, completely customizable, performant and 100% free
    blog template for Jekyll built by
    <a href="/about" title="About me">Nielsen Ramon</a>. Download it <a href="https://github.com/nielsenramon/chalk" rel="noreferrer noopener" target="_blank" title="Download Chalk">here</a>.
  </p>
</footer>

      </div>
    </div>
  </main>
  
  <script>
    window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
    ga('create','UA-28631876-6','auto');ga('send','pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async></script>


<script src="/assets/vendor.js"></script>



  <script src="/assets/webfonts.js"></script>




  <script src="/assets/scrollappear.js"></script>



<script src="/assets/application.js"></script>


</body>
</html>
