<!doctype html>
<html lang="en">

<head>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The Art of Machine Learning</title>
  <meta name="description" content="Machine Learning, Deep Learning, Jenny Yu ">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Neural Network for type 3 alcoholism classifier">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://yinniyu.github.io/posts/snp_ann.html">
  <meta property="og:site_name" content="The Art of Machine Learning">
  <meta property="og:image" content="http://chalk.nielsenramon.com/assets/documentation/sample-image.jpg">

 
  <link rel="apple-touch-icon" href="/assets/apple-touch-icon.png">
  <link href="http://chalk.nielsenramon.com/feed.xml" type="application/rss+xml" rel="alternate" title="Chalk Last 10 blog posts" />

    <link rel="stylesheet" href="/assets/light.css">

  
</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav appear">
  <a href="/" class="header-logo" title="The Art of Machine Learning">The Art of Machine Learning</a>
  <ul class="header-links">

     <li>
        <a href="https://github.com/yinniyu" rel="noreferrer noopener" target="_blank" title="GitHub">
          <span class="icon icon-github"></span>
        </a>
      </li>
    
    
      <li>
        <a href="https://linkedin.com/in/jenny-yu-b495243" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <span class="icon icon-linkedin"></span>
        </a>
      </li>   
  </ul>
</nav>

        
        <article class="article appear">
          <header class="article-header">
            <h1>Sparse PCA and XGboost classifier for predicting tumor type </h1>
            <p>High throughput RNA sequencing data is used to build a classifier for 5 tumor types: breast cancer, lung cancer,
            colorectal cancer, kidney cancer and prostate cancer. This public data was provided and maintained by the 
            <a href="https://portal.gdc.cancer.gov/">The Cancer Genome Atlas (TCGA) Research Network</a>.</p>
            
            <div class="article-list-footer">
              <span class="article-list-date">
                Nov 20, 2017
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
              10 minute read
              </span>
              
              <span class="article-list-divider">-</span>
              <div class="article-list-tags">
                
                <span class="article-list-divider">R, Sparse PCA, Gene expression, xgboost </span>
              </div>
            </div>
          </header>

          <div class="article-content">

<h2 id="Loci">Genomic profiling of cancer (MDD)</h2>


<p>\
    </p>
            
<h2 id="data processing">Getting the genotype data</h2>

<p></p>

 <figure>
   <img src="images/rna_seq/class_distribution.png" alt="tumor distribution in data" >
   <figcaption>Fig 1. Count distribution of tumor types in 801 samples. BRCA-breast cancer,COAD-Colorectal cancer, KIRC-Kidney cancer, LUAD- Lung cancer,
    PRAD- Prostate cancer.
   </figcaption>
   </figure>
  
<p>In my colleague's study, they excluded Lesche Type IV subjects due to potential confounding factors, and focused on the females
  because past literature found that Type III alcoholism predominanty affects female. After QC, the sample size for statistical analysis
  and logistic regression was 116. In building a classifier, I decided to include the male group since they made up 60% of the subjects,
  and I see that there's quite a few male subjects in the Type III group within this study(Fig 2). This brings the sample size to 293.</p>

<figure>
   <div>
    <a href="https://plot.ly/~jypucca/2/?share_key=YRmcUqgqP0CZ02GCRwjcCY" target="_blank" title="pca_plot" style="display: block; text-align: center;"><img src="https://plot.ly/~jypucca/2.png?share_key=YRmcUqgqP0CZ02GCRwjcCY" alt="pca_plot" style="max-width: 100%;width: 600px;"  width="600" onerror="this.onerror=null;this.src='https://plot.ly/404.png';" /></a>
    <script data-plotly="jypucca:2" sharekey-plotly="YRmcUqgqP0CZ02GCRwjcCY" src="https://plot.ly/embed.js" async></script>
</div>
  <div>
    <a href="https://plot.ly/~jypucca/4/?share_key=R6VcHxKIJYVKDpQvY7DLIy" target="_blank" title="pca_plot_sparse1000" style="display: block; text-align: center;"><img src="https://plot.ly/~jypucca/4.png?share_key=R6VcHxKIJYVKDpQvY7DLIy" alt="pca_plot_sparse1000" style="max-width: 100%;width: 600px;"  width="600" onerror="this.onerror=null;this.src='https://plot.ly/404.png';" /></a>
    <script data-plotly="jypucca:4" sharekey-plotly="R6VcHxKIJYVKDpQvY7DLIy" src="https://plot.ly/embed.js" async></script>
</div>
   <figcaption>Fig 2. Top- 3D plot of principal components of tradition PCA (prcomp library). Bottom-3D plot of sparse PCA (nspca library).
   </figcaption>
   </figure>


<h2 id="Neural Network">Neural Network</h2>


<p>Using the cleaned-up dataset, I will compare model performance of Logistic Regression with Neural Network using the R library <i>Neuralnet</i>.
       Neural network is a machine learning model inspired by neuronal activation and transmission of signal in the brain.
        I like to think of it as a fully connected network of 'nodes' that introduce nonlinearity to the weighted inputs by applying some kind of 
       activation function (i.e., sigmoid, tanh, ReLU functions) before propagating the outputs to the next layer (Fig 3).The input layer 
  corresponds to the feature variables of the data (SNP features, gender class). For each node in the hidden layer, the dot product 
  of the input and weights are fed into the activation function. The nodes in the output layer correspond to the number of classes. In my case, a binary classification (Type III or Non Type III) 
            results in only one node in the final output layer.</p>

<figure>
   <img src="images/snp_array/neural_net2.jpeg" alt="neural network" >
   <figcaption>Fig 3. A simple NN architechture. <i>Source: https://cs231n.github.io/neural-networks-1/</i>
   </figcaption>
   </figure>


<h2 id="randomeforest">Caret and the Forest</h2>

<p>After the data are preprocessed, I used the <i> Caret</i>  library to perform grid search with cross validation (CV). I chose 
<a href="https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html">Random Forest (RF) regressor </a> because it is known to work well with nonlinear data, it selects random 
subset of features at each node split, and it reduces variance (aka: overfitting) of the model by having independent weak learner trees. </p>
<p>Since the trees in RF are independent of each other, it is easy to setup parallel computing.
This is helpful for running RF on large dataset with ~1M samples. On the <i> Caret</i>  documentation, the author only mentioned 
<i> doMC</i>  for parallel computing but that only works for Macs. I have a windows 10 laptop with 4 cores (8 processors), and found that
the <i>doParallel</i>  package worked well with the CV workflow.
<figure class="highlight"><pre><code class="language-R" data-lang="R">
library(randomForest)
library(caret)
library(parallel)
library(doParallel)
library(dplyr)
datasets<- datasets%>% mutate_if(is.integer, as.numeric)
trainIndex <- createDataPartition(datasets$log_duration, p = .7, 
                                  list = FALSE, 
                                  times = 1)
data_train<-datasets[trainIndex,]
data_val<-datasets[-trainIndex,]
#### grid must include mtry for RF model ####
rfgrid<- expand.grid(mtry=c(6,13))
set.seed(20)
cv_control <- trainControl(
  method = "cv",
  number = 3)
  
###### NOTE: CV and grid functions has to come before cluster computing setup  ##########

cluster <- makeCluster(detectCores()-3)# leave at least 1 node for operating system 
registerDoParallel(cluster)

##train function will invoke dummy variables for formula input!
##but randomForest library itself doesn't do that with formula input
rfFit <- train(log_duration ~ ., data = data_train, 
                 method = "rf", ntree=100,
                 trControl = cv_control, 
                 tuneGrid=rfgrid,
                 verbose = FALSE, importance=TRUE)

stopCluster(cluster)
registerDoSEQ()
## to see error of the model
rfFit$results
</code></pre></figure>

<p> Running the model fit on only 400K samples (to save time), the average validation RMSE was around 0.4135. There's still 
room for improvement, especially by running the whole training set. As you can see, it's easy to perform cross validated parameter
tuning on a Random Forest regressor through <i>Caret</i>, which comes with other widely used models such as SVM, Logistic Regression, 
and GLM regression, etc (total of 238).
        


          
            <div id="disqus_thread" class="article-comments"></div>
            <script src="https://chalk-1.disqus.com/embed.js" async defer></script>
            <noscript>Please enable JavaScript to view the comments.</noscript>
          
        </article>
        <footer class="footer appear">
  <p>
    Chalk is a high quality, completely customizable, performant and 100% free
    blog template for Jekyll built by
    <a href="/about" title="About me">Nielsen Ramon</a>. Download it <a href="https://github.com/nielsenramon/chalk" rel="noreferrer noopener" target="_blank" title="Download Chalk">here</a>.
  </p>
</footer>

      </div>
    </div>
  </main>
  
  <script>
    window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
    ga('create','UA-28631876-6','auto');ga('send','pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async></script>


<script src="/assets/vendor.js"></script>



  <script src="/assets/webfonts.js"></script>




  <script src="/assets/scrollappear.js"></script>



<script src="/assets/application.js"></script>


</body>
</html>
