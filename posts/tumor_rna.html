<!doctype html>
<html lang="en">

<head>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The Art of Machine Learning</title>
  <meta name="description" content="Machine Learning, Deep Learning, Jenny Yu ">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Neural Network for type 3 alcoholism classifier">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://yinniyu.github.io/posts/snp_ann.html">
  <meta property="og:site_name" content="The Art of Machine Learning">
  <meta property="og:image" content="http://chalk.nielsenramon.com/assets/documentation/sample-image.jpg">

 
  <link rel="apple-touch-icon" href="/assets/apple-touch-icon.png">
  <link href="http://chalk.nielsenramon.com/feed.xml" type="application/rss+xml" rel="alternate" title="Chalk Last 10 blog posts" />

    <link rel="stylesheet" href="/assets/light.css">

  
</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav appear">
  <a href="/" class="header-logo" title="The Art of Machine Learning">The Art of Machine Learning</a>
  <ul class="header-links">

     <li>
        <a href="https://github.com/yinniyu" rel="noreferrer noopener" target="_blank" title="GitHub">
          <span class="icon icon-github"></span>
        </a>
      </li>
    
    
      <li>
        <a href="https://linkedin.com/in/jenny-yu-b495243" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <span class="icon icon-linkedin"></span>
        </a>
      </li>   
  </ul>
</nav>

        
        <article class="article appear">
          <header class="article-header">
            <h1>Sparse PCA and XGboost classifier for predicting tumor type </h1>
            <p>The Pan-Cancer intiative at <a href="https://portal.gdc.cancer.gov/">The Cancer Genome Atlas (TCGA) Research Network</a> 
              has analyzed numerous tumor types using different technologies to profile the genomic, transcriptomic and epigenomic variations.
              This body of work represents an approach to study cancers not only based on the organ of origin, but also the broader biomarker 
              profiles [1]. For this post, high-throughput gene expression data are used to build a classifier for 5 tumor types: breast cancer, lung cancer,
            colorectal cancer, kidney cancer and prostate cancer. The public data was provided and maintained by TCGA, The raw RNA sequence data were 
              normalized to Fragment per kilobase transcript per million mapped reads (FPKM) to adjust for differences of transcript lengths.
              The normalized data (801 samples,20,531 features for gene expression level) are compiled and hosted by 
              <a href="https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq#">UCI Machine Learning Repository</a>.</p>
            
            <div class="article-list-footer">
              <span class="article-list-date">
                Dec 4th, 2017
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
              10 minute read
              </span>
              
              <span class="article-list-divider">-</span>
              <div class="article-list-tags">
                
                <span class="article-list-divider">R, Sparse PCA, Gene expression, xgboost </span>
              </div>
            </div>
          </header>

          <div class="article-content">

<h2 id="preprocessing">Data preprocessing</h2>

<p>As shown in Fig.1, this data set is imbalanced with Colorectal cancer being underpresented. Taking this into account for multiclass
  classification, F1 score will be used that reflects the precision and recall performance within each class. There's also the
  issue of high dimensionality in this data, where number of features (20,531 columns) far exceeds the number of samples (801 rows).
  A modified form of Principal Component Analysis (PCA) will be used to reduce the complexity of the dataset.</p>

 <figure>
   <img src="images/rna_seq/class_distribution.png" alt="tumor distribution in data" >
   <figcaption>Fig.1. Count distribution of tumor types in 801 samples. BRCA-breast cancer,COAD-Colorectal cancer, KIRC-Kidney cancer, LUAD- Lung cancer,
    PRAD- Prostate cancer.
   </figcaption>
   </figure>
  
<p> For a quick refresher on the theoretical frame work of Prinicipal Component Analysis (PCA), I highly recommend <a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues">
  this link</a>. Briefly, PCA is a technique commonly used to reduce high dimensional numerical data to lower dimensional space.
   Traditional PCA works by combining all original features with linear weights (aka: loadings) to capture the maximum variance 
  of the data. The analysis results in several principal components, which are ranked by the amount of variance it captures. 
  Each Principal Component(PC) will have its own eigenvector and eigenvalues that characterizes the variance, and each PC will have its own loadings 
  for the features. For example, to calculate the first PC score for one sample, let <i>C<sub>i,j</sub></i> be the loadings for <i>i-th</i> PC 
  and <i>jth</i> <a href="https://izabelcavassim.files.wordpress.com/2015/03/screenshot-from-2015-03-08-2245511.png">FPKM-normalized </a>
    gene expression feature:
  <br>PC1 Score = gene_measurement{1} x C<sub>1,1</sub></i> +
            gene_measurement{2} x C<sub>1,2</sub></i> +...
            gene_measurement{j} x C<sub>1,j</sub></i></br>


<p>Using the cleaned-up dataset, I will compare model performance of Logistic Regression with Neural Network using the R library <i>Neuralnet</i>.
       Neural network is a machine learning model inspired by neuronal activation and transmission of signal in the brain.
        I like to think of it as a fully connected network of 'nodes' that introduce nonlinearity to the weighted inputs by applying some kind of 
       activation function (i.e., sigmoid, tanh, ReLU functions) before propagating the outputs to the next layer (Fig 3).The input layer 
  corresponds to the feature variables of the data (SNP features, gender class). For each node in the hidden layer, the dot product 
  of the input and weights are fed into the activation function. The nodes in the output layer correspond to the number of classes. In my case, a binary classification (Type III or Non Type III) 
            results in only one node in the final output layer.</p>

<figure>
   <img src="images/snp_array/neural_net2.jpeg" alt="neural network" >
   <figcaption>Fig 3. A simple NN architechture. <i>Source: https://cs231n.github.io/neural-networks-1/</i>
   </figcaption>
   </figure>


<h2 id="randomeforest">Caret and the Forest</h2>

<p>After the data are preprocessed, I used the <i> Caret</i>  library to perform grid search with cross validation (CV). I chose 
<a href="https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html">Random Forest (RF) regressor </a> because it is known to work well with nonlinear data, it selects random 
subset of features at each node split, and it reduces variance (aka: overfitting) of the model by having independent weak learner trees. </p>
<p>Since the trees in RF are independent of each other, it is easy to setup parallel computing.
This is helpful for running RF on large dataset with ~1M samples. On the <i> Caret</i>  documentation, the author only mentioned 
<i> doMC</i>  for parallel computing but that only works for Macs. I have a windows 10 laptop with 4 cores (8 processors), and found that
the <i>doParallel</i>  package worked well with the CV workflow.
<figure class="highlight"><pre><code class="language-R" data-lang="R">
library(randomForest)
library(caret)
library(parallel)
library(doParallel)
library(dplyr)
datasets<- datasets%>% mutate_if(is.integer, as.numeric)
trainIndex <- createDataPartition(datasets$log_duration, p = .7, 
                                  list = FALSE, 
                                  times = 1)
data_train<-datasets[trainIndex,]
data_val<-datasets[-trainIndex,]
#### grid must include mtry for RF model ####
rfgrid<- expand.grid(mtry=c(6,13))
set.seed(20)
cv_control <- trainControl(
  method = "cv",
  number = 3)
  
###### NOTE: CV and grid functions has to come before cluster computing setup  ##########

cluster <- makeCluster(detectCores()-3)# leave at least 1 node for operating system 
registerDoParallel(cluster)

##train function will invoke dummy variables for formula input!
##but randomForest library itself doesn't do that with formula input
rfFit <- train(log_duration ~ ., data = data_train, 
                 method = "rf", ntree=100,
                 trControl = cv_control, 
                 tuneGrid=rfgrid,
                 verbose = FALSE, importance=TRUE)

stopCluster(cluster)
registerDoSEQ()
## to see error of the model
rfFit$results
</code></pre></figure>

<p> Running the model fit on only 400K samples (to save time), the average validation RMSE was around 0.4135. There's still 
room for improvement, especially by running the whole training set. As you can see, it's easy to perform cross validated parameter
tuning on a Random Forest regressor through <i>Caret</i>, which comes with other widely used models such as SVM, Logistic Regression, 
and GLM regression, etc (total of 238).
        
<h3> REFERENCE </h3>
<p>[1].The Cancer Genome Atlas Research Network, Weinstein, J., Collisson, E.A., Mills,G.B., et al. <i>The Cancer Genome Atlas 
Pan-Cancer analysis project</i>,Nature Genetics 45, 1113â€“1120 (2013)

          
            <div id="disqus_thread" class="article-comments"></div>
            <script src="https://chalk-1.disqus.com/embed.js" async defer></script>
            <noscript>Please enable JavaScript to view the comments.</noscript>
          
        </article>
        <footer class="footer appear">
  <p>
    Chalk is a high quality, completely customizable, performant and 100% free
    blog template for Jekyll built by
    <a href="/about" title="About me">Nielsen Ramon</a>. Download it <a href="https://github.com/nielsenramon/chalk" rel="noreferrer noopener" target="_blank" title="Download Chalk">here</a>.
  </p>
</footer>

      </div>
    </div>
  </main>
  
  <script>
    window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
    ga('create','UA-28631876-6','auto');ga('send','pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async></script>


<script src="/assets/vendor.js"></script>



  <script src="/assets/webfonts.js"></script>




  <script src="/assets/scrollappear.js"></script>



<script src="/assets/application.js"></script>


</body>
</html>
