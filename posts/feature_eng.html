<!doctype html>
<html lang="en">

<head>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The Art of Machine Learning</title>
  <meta name="description" content="Machine Learning, Deep Learning, Jenny Yu, denver ">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Feature Engineering - getting the most out of your data">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://jyu-theartofml.github.io/posts/feature_eng.html">
  <meta property="og:site_name" content="The Art of Machine Learning">
  <meta property="og:image" content="images/feature_eng/feat_eng.jpg">


  <link rel="apple-touch-icon" href="/assets/apple-touch-icon.png">
  <link href="http://chalk.nielsenramon.com/feed.xml" type="application/rss+xml" rel="alternate" title="Chalk Last 10 blog posts" />

    <link rel="stylesheet" href="/assets/light.css">


</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav appear">
  <a href="/" class="header-logo" title="The Art of Machine Learning">The Art of Machine Learning</a>
  <ul class="header-links">

    <li>
        <a href="https://jyu-theartofml.github.io/about">About Me
        </a>
      </li>

    <li>
        <a href="https://jyu-theartofml.github.io/toolbox">My Toolbox</a>
      </li>
    <li>
        <a href="https://jyu-theartofml.github.io/ai_art">AiArt
        </a>
      </li>
     <li>
        <a href="https://github.com/jyu-theartofml" rel="noreferrer noopener" target="_blank" title="GitHub">
          <span class="icon icon-github"></span>
        </a>
      </li>


      <li>
        <a href="https://linkedin.com/in/jenny-yu-co-80111" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <span class="icon icon-linkedin"></span>
        </a>
      </li>
  </ul>
</nav>


        <article class="article appear">
          <header class="article-header">
            <h1>Feature Engineering - getting the most out of your data</h1>
            <figure>
              <img src='images/feature_eng/feat_eng.jpg' alt="feature sketch" />
               <figcaption>Feature Engineering for Machine Learning (source: <a href='https://www.udemy.com/feature-engineering-for-machine-learning'><u>Udemy</u></a>).</figcaption></figure>
            <p>Throughout my experience in Data Science, I think feature engineering is the one practical skill that didn't receive enough emphasis in textbook or
              other formal channel of learning (Docker is the other one). It wasn't until I started participating in Kaggle competitions when I realize the value of feature engineering in building
              machine learning models.
              Overall, feature engineering is a way to extract more insights from the data based on some domain knowledge (although it's been shown in
              some cases domain knowledge is not required), in a way that would improve the model's predictive power. This blog post will cover a few of the methods
              that I found to be useful.
              </p>

            <div class="article-list-footer">
              <span class="article-list-date">
                May 1st, 2019
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
              10 minute read
              </span>

              <span class="article-list-divider">-</span>
              <div class="article-list-tags">

                <span class="article-list-divider">R, Python, Feature Engineering</span>
              </div>
            </div>
          </header>

          <div class="article-content">


<h2 id="Basic FE">The Nature of the Feature</h2>

<p>Sometimes, people are performing basic feature engineering without realizing that it is feature engineering. Technically speaking, z-score standardization of
continuous variables is a type of feature engineering for many ML models. Another example would be extracting time elements like
month, or day of the week from timestamps. In the former case, normalizing the data helps the model learn more efficiently if it is updating the
the weights using gradient descend; in the latter, it is extracting additional variables from the raw data.</p>

<p>Speaking of data normalization, this is where having some domain knowledge can help. For an economic dataset, values are often normalized to per-capita basis, or adjusted for inflation.
  If you are working in the realm of public health, data like death rate at the state level is age adjusted based on that state's population. For gene expression analysis, it's
    common to normalize sequence data to Fragment per kilobase transcript per million mapped reads (FPKM). Aside from normalization, some dataset
    might also have a time lag element. For example, some studies show that infectious disease like malaria in East Africa
    is correlated with accumulated precipitation from the past 2-3 months.
  </p>
  <p>In working with categorical variables, having many unique values for one feature can introduce undesirable 'noise' to the model. A perfect
    example of this is the Animal Shelter dataset from Kaggle, where the number of dog breeds listed was about 150. One approach for feature engineering was
    to simply map the dog breed to a more general type (Herding, Hound, Mix, Non-sporting, Sporting, etc)
    and also to different size groups(small-large). For the cats, the top 5 breeds actually made
    up 96% of population, so the other cat breeds were grouped into one category called "other". In this case, binning categorical
    variables helped the model to learn more effectively and made a significant difference in performance.</p>

<h2 id="Numerical data">Checking the Assumptions</h2>
<p>Most of the machine learning models assume a normal(ish) distribution for continuous variables, so generally speaking it's nice to have a numerical variable
with a distribution that is close to normal. If your data has a somewhat skewed distribution, a logarithmic transformation would usually be adequate in yielding a
normal curve. However, sometimes the distribution is SO skewed that a simple log function won't do the job, and you might need the <b>Box Cox</b> transformation.</p>
<figure>
   <p align='center'><img src="images/feature_eng/boxcox-formula.png" alt="boxcox" ></p>
   </figure>
<p>This transformation tests a range of values for the hyperparameter λ (usually -3 to 3), and the optimal λ is chosen based on how closely it can approximate a
  normal distribution curve (when λ=0 it's log ). This function offers more flexibility than the log or exponential function.</p>

<p>Another thing to check is multicollinearity (a.k.a, auto-correlation). Models like Naive Bayes classifier and GLM regression assume the
  predictor variables are independent of each other. If that's not the case, then the model won't work or it would be highly unstable (i.e, high variance
  in the coefficients for regression). An example of multicollinearity is when you have variable X, and there's a second variable with value X^2. These variables are not
  independent of each other, and having the two together would 'confuse' the model during training therefore making it unstable. A typical solution
  is to drop one of the correlated variables. In Python, you can figure out which variables have multicollinearity using the function
  <code class="highlighter-rouge">variance_inflation_factor</code> from <code class="highlighter-rouge">statsmodels.stats.outliers_influence </code>.
  For R users, it's the <code class="highlighter-rouge"> VIF</code> library. If the returned VIF value is 5-10, then it's likely there's multicollinearity[1].
  It's always a good practice to resolve multicollinearity issues, especially before training on regression models.</p>

  <p>But what if you have lots of variables to sift through? It would be impractical to manually inspect the VIF and decide which auto-correlated variables
  to omit. </p>
  <p>This is where Principal Component Analysis (PCA) comes in as a remedy. In capturing the maximum variance of the data, PCA reduces the
  variable dimensions into uncorrelated components, which effectively remove the effect of multicollinearity. There's a detailed explanation of how PCA
  works in an old <a href='https://jyu-theartofml.github.io/posts/tumor_rna'> post</a>. If working with categorical data and dummy variables,
   use <a href='https://pypi.org/project/mca/'> Multiple Correspondence Analysis (MCA)</a> to reduce the high dimensions. For R, the MCA function
   can be found in library such as <code class="highlighter-rouge">FactoMineR</code>.
</p>


<h2 id="missing data">Missing Data</h2>

<p>Depending on how you want to approach missing data, it can be a type of feature engineering. I
  had written a blog <a href='https://jyu-theartofml.github.io/posts/imputation'>post </a> just on that topic alone.</p>

<h2 id="dim reduction">Trimming the predictors</h2>
<p>Often you might want to reduce the dimensions of the variable, especially when the number of features is greater than training sample size.
  As mentioned above, PCA is a good way to go, however it's difficult to interpret each derived components from PCA. Another option to reduce the
  high dimension of data before training is to use <a href='https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso'>
    Lasso regression</a>, which selects features by shrinking some of the coeffients to 0 through a L1-norm penalty. How many feature it selects
  depends on the hyperparameter alpha that determines how aggressive the shrinkage would be.</p>
  <figure>
     <p align='center'><img src="images/feature_eng/l1_norm.jpeg" alt="lasso_coeff" width=70% height=70%></p>
     <figcaption><p align='center'>Fig 1. Lasso shrinkage of coefficients as a function of L1-norm (source: <a href='https://www.r-bloggers.com/lasso-regression-in-r-exercises/'> r-bloggers</a>).</p>
     </figcaption>
     </figure>

<p>You might be wondering which of these two methods is better suited for your model. It depends on the research question you are trying to answer.
 With Lasso regression, the assumption is that few features can adequately explain the outcome variable, and those features are independent of each other.
 If you have correlation between two variables, Lasso will very likely drop one of them. For traditional PCA, it is not dropping the
 individual variables(unless it's sparse PCA), but 'summarizing' the variables into a component through linear combination. Therefore, there's no explicit assumptions with
 PCA transformation, and it's well suited for dataset in which you believe each of the many variables have relatively small contribution to
 the overall variance. Whereas using Lasso regression assumes that only a few variables have the largest impact on the outcome.</p>

 <h2 id='automate FE'>Automate new features</h2>
 <p>In the real world, you will often have data that's spread across multiple tables and are linked by a unique identifier. There's
   a Python library called <b><a href='https://github.com/Featuretools/featuretools'> Featuretools</b></a> that can automate
   feature creations giving more options for downstream feature selection. The idea behind this concept is simple, it stacks multiple transformation
   and aggregation operation (called <i>feature primitives</i> in the package) using data shared by different tables[2]. With this package,
   you can create additional features such as <code class="highlighter-rouge">Mean</code>,<code class="highlighter-rouge">Min</code>,
   <code class="highlighter-rouge">Max</code>, <code class="highlighter-rouge">AvgTimeBetween</code>,
   <code class="highlighter-rouge">Skew</code>, <code class="highlighter-rouge">PercentTrue</code>, and etc for a variable of interest.
   The user can also perform more than 1 level of stacking to generate more granular features. This type of methodology can be useful
   in feature engineering without domain knowledge, for instance when the column names are encripted.</p>
   <p>There's also a package called <b><a href='https://github.com/WillKoehrsen/feature-selector'>FeatureSelector</b></a> for doing routine selection
     based on criterias like percentage of missing values, multicollinearity, feature importance, etc. It's a handy module that removes highly-correlated
     variables, and identifies features with zero or low importance based on a LightGBM model[3].</p>

  <h2>Engineering for better models</h2>
  <p>All of the techniques and methods mentioned above are practical implementations that can help increase the accuracy of many
    ML models, and potentially improve model explainability. It also goes to show that feature engineering is an art form that requires some degree of creativity. One could
    even argue that feature engineering is what sets a good model apart from a mediocre one, because it's easy to find open source code that
    shows how to train a model and return some results, but more challenging to mine the right features from the raw data.</p>


  <h3>REFERENCES</h3>
  <p>[1]Tavares III, E. <i> Variance Inflation Factor (VIF) Explained</i>, https://etav.github.io/python/vif_factor_python.html></p>
  <p>[2]Featuretools Documentation, https://docs.featuretools.com/automated_feature_engineering/afe.html</p>
  <p>[3]Koehrsen, W. <i>A Feature Selection Tool for Machine Learning in Python</i>, https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0</p>





            <div id="disqus_thread" class="article-comments"></div>
            <script src="https://chalk-1.disqus.com/embed.js" async defer></script>
            <noscript>Please enable JavaScript to view the comments.</noscript>

        </article>
        <footer class="footer appear">


    Chalk is a high quality, completely customizable, performant and 100% free
    blog template for Jekyll built by
    <a href="/about" title="About me">Nielsen Ramon</a>. Download it <a href="https://github.com/nielsenramon/chalk" rel="noreferrer noopener" target="_blank" title="Download Chalk">here</a>.
  </p>
</footer>

      </div>
    </div>
  </main>

  <script>
    window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
    ga('create','UA-28631876-6','auto');ga('send','pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async></script>


<script src="/assets/vendor.js"></script>



  <script src="/assets/webfonts.js"></script>




  <script src="/assets/scrollappear.js"></script>



<script src="/assets/application.js"></script>


</body>
</html>
