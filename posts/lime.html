<!doctype html>
<html lang="en">

<head>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The Art of Machine Learning</title>
  <meta name="description" content="Machine Learning, Deep Learning, Jenny Yu ">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Looking Deeper with LIME">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://jyu-theartofml.github.io/posts/lime.html">
  <meta property="og:site_name" content="The Art of Machine Learning">
  <meta property="og:image" content="https://jyu-theartofml.github.io/posts/images/lime/LIME_features.png">


  <link rel="apple-touch-icon" href="/assets/apple-touch-icon.png">
  <link href="http://chalk.nielsenramon.com/feed.xml" type="application/rss+xml" rel="alternate" title="Chalk Last 10 blog posts" />

    <link rel="stylesheet" href="/assets/light.css">


</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav appear">
  <a href="/" class="header-logo" title="The Art of Machine Learning">The Art of Machine Learning</a>
  <ul class="header-links">

    <li>
        <a href="https://jyu-theartofml.github.io/about">About Me
        </a>
      </li>

    <li>
        <a href="https://jyu-theartofml.github.io/toolbox">My Toolbox</a>
     </li>

    <li>
        <a href="https://jyu-theartofml.github.io/ai_art">AiArt
        </a>
      </li>

    <li>
        <a href="https://github.com/jyu-theartofml" rel="noreferrer noopener" target="_blank" title="GitHub">
          <span class="icon icon-github"></span>
        </a>
      </li>


      <li>
        <a href="https://linkedin.com/in/jenny-yu-co-80111" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <span class="icon icon-linkedin"></span>
        </a>
      </li>
  </ul>
</nav>


        <article class="article appear">
          <header class="article-header">
            <h1>Looking deeper with LIME</h1>
             <figure>
              <img src='images/lime/localized_linear.png' alt="lime_local" width='80%' height='80%'>
               <figcaption>Source:Ribeiro et al. <i>Why should I trust you? Explaining the predictions of any classifier.</i></figcaption></figure>
            <p>Many AI 'black boxes'
            are based on some flavors of
            neural network or deep learning building blocks, which makes model interpretability a challenging task. While the
            stakes are low if a convolutional neural net misclassifies a Shar Pei for a bath towel, it's a different beast if AI is
            performing automated medical diagnostics, making decisions on credit applications, or even finding <a href='https://en.wikipedia.org/wiki/Person_of_Interest_(TV_series)'>
              <u> 'Person of Interest' </u></a>. The new EU general data protection
            regulation (GDPR), effective May 25th, 2018, requires a "right to explanation" for human subjects of AI/automated system[1].
            This means a subject/consumer has the right to obtain an explanation of the automated decision, and the right to opt out of
            a decision based solely on AI/automated algorithm that produces legal effects on them (i.e, job recruiting,
            loan applications) without human intervention. Back in 2016, researchers (Ribeiro et al) from University of
            Washington published an algorithm called LIME that addresses the need for model interpretability and evaluation by
              explaining the predictions of any classifier in an
            intuitive manner. In this post, I will apply LIME to assess the DenseNet melanoma classifier by visualizing feature importance.


            <div class="article-list-footer">
              <span class="article-list-date">
                April 30th, 2018
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
              10 minute read
              </span>

              <span class="article-list-divider">-</span>
              <div class="article-list-tags">

                <span class="article-list-divider">Python, Model Evaluation, LIME, DeepLearning </span>
              </div>
            </div>
          </header>

          <div class="article-content">


<h2>LIME - a model of a model</h2>

<p> Local Interpretable Model-Agnostic Explanations (LIME) is an algorithm that explains a given prediction by approximating
a separate interpretable model locally around the prediction[2]. To quote the lead author Marco Ribeiro:

<blockquote class="w3-panel w3-leftbar w3-light-grey">
  " <b>Local </b>refers to local fidelity - i.e., we want the explanation to really reflect the behaviour of the classifier
  'around' the instance being predicted. This explanation is useless unless it is <b>interpretable</b> - that is,
  unless a human can make sense of it.
  Lime is able to explain any model without needing to 'peek' into it, so it is <b>model-agnostic</b>."
</blockquote>

This post won't delve into the mathematical details (they are in the published paper if you are interested). The picture at
            the top illustrates fitting a localized weighted linear model for a given prediction.
            For image classification, LIME
            would perturb the original image to generate a data set. For each perturbed sample, it calculates the
            probability that the image belongs to a given class according to the model. Then it learns a locally faithful
            linear model,
            giving weights to perturbed data points by their proximity to the original image (Fig.1.). It's important to stress that LIME
            generates a separate model that attempts to explain the prediction of the original model. The LIME model
            approximates a local linear model around the vicinity of the original sample being explained.

<figure>
    <center><img src="images/lime/LIME_example.jpg" alt="frog and lime" ></center>
   <figcaption>Fig.1.Explaining feature importance of a frog thru LIME. (source:https://www.kdnuggets.com/2016/08/introduction-local-interpretable-model-agnostic-explanations-lime.html)  </figcaption>
   </figure>

  <h2>What did the model learn? </h2>
    <p>After importing the <a href='https://github.com/jyu-theartofml/ISIC_melanoma'>trained DenseNet model </a> and test images, I used
      LIME to evaluate the model and generate a heat map image
      superimposed on the original image.
            The code is show below:</p>

 <figure class="highlight"><pre><code class="language-Python" data-lang="Python">
import lime
from lime import lime_image
from skimage.segmentation import mark_boundaries
import skimage
from keras.preprocessing.image import ImageDataGenerator,array_to_img, img_to_array, load_img, array_to_img
from keras.applications.densenet import preprocess_input

explainer = lime_image.LimeImageExplainer()

###num_samples – size of the neighborhood (perturbed instances) to learn an interpretable model
explanation = explainer.explain_instance(test_preprocessed[2], model.predict, labels=['benign','melanoma','sk'], top_labels=3, hide_color=0, num_samples=200)
temp, mask = explanation.get_image_and_mask(0, positive_only=False, num_features=10, hide_rest=False)
plt.imshow(mark_boundaries(array_to_img(temp), mask))
</code></pre></figure>


<p> The image output from the codes are shown in Fig.2. The green pixels represent features that are <i>for </i>
  the prediction of that class, and red
  pixels means they are
  <i>against</i> the prediction. After looking through some misclassified samples, I noticed that they are similar to the right side image in
  Fig.2.(false negative) - they consist of smaller lesions relative to the rest. This suggests that the model struggled to learn
  relevant features such as edges and uneven
  colorations (perhaps due to the resolution of such small region of interest). It also seems to think that the black background pixel is
  somehow associated with a benign prediction.</p>

  <figure>
    <center><img src="images/lime/LIME_features.png" alt="melanoma and lime" ></center>
   <figcaption>Fig.2. Explanation of the prediction suggests that DenseNet model overfits by learning on background pixels. </figcaption>
   </figure>

 <h2>Potential ways to improve</h2>
   <p>One way to help the DenseNet model learn better is by training on more images of small melanoma lesions (distribution of data
     matters), and trying different augmentation techniques for preprocessing. LIME has provided interesting insights into the
    flaws of the melanoma classifier, allowing the user to evaluate and improve 'black box' model with an intuitive understanding.
     Besides image prediction,
     LIME also works with text or categorical data for explaining classifier models (but not regression). And it's also
     <a href='https://www.data-imaginist.com/2017/announcing-lime/'>available in R </a>. Check out the video below if you are
     interested in learning more.</p>

 <div class="embed-responsive embed-responsive-16by9">
   <iframe width="560" height="315" src="https://www.youtube.com/embed/hUnRCxnydCc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>

<h3> REFERENCES </h3>
            <p>[1]Wu, P., <i>GDPR and its impacts on machine learning applications </i>, Medium.com (https://medium.com/trustableai/gdpr-and-its-impacts-on-machine-learning-applications-d5b5b0c3a815) </p>
  <p>[2]Ribeiro et al. <b>Why should I trust you? Explaining the predictions of any classifier</b>, https://arxiv.org/pdf/1602.04938.pdf </p>

 <div id="disqus_thread" class="article-comments"></div>
<script src="https://chalk-1.disqus.com/embed.js" async defer></script>
<noscript>Please enable JavaScript to view the comments.</noscript>

</article>
<footer class="footer appear">
  <p>
    Chalk is a high quality, completely customizable, performant and 100% free
    blog template for Jekyll built by
    <a href="/about" title="About me">Nielsen Ramon</a>. Download it <a href="https://github.com/nielsenramon/chalk" rel="noreferrer noopener" target="_blank" title="Download Chalk">here</a>.
  </p>
</footer>

      </div>
    </div>
  </main>

  <script>
    window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
    ga('create','UA-28631876-6','auto');ga('send','pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async></script>


<script src="/assets/vendor.js"></script>



  <script src="/assets/webfonts.js"></script>




  <script src="/assets/scrollappear.js"></script>



<script src="/assets/application.js"></script>


</body>
</html>

    © 2018 GitHub, Inc.
    Terms
    Privacy
    Security
    Status
    Help

    Contact GitHub
    API
    Training
    Shop
    Blog
    About
