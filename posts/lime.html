<!doctype html>
<html lang="en">

<head>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The Art of Machine Learning</title>
  <meta name="description" content="Machine Learning, Deep Learning, Jenny Yu ">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Looking Deeper with LIME">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://yinniyu.github.io/posts/lime.html">
  <meta property="og:site_name" content="The Art of Machine Learning">
  <meta property="og:image" content="images/lime/localized_linear.png">

 
  <link rel="apple-touch-icon" href="/assets/apple-touch-icon.png">
  <link href="http://chalk.nielsenramon.com/feed.xml" type="application/rss+xml" rel="alternate" title="Chalk Last 10 blog posts" />

    <link rel="stylesheet" href="/assets/light.css">

  
</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav appear">
  <a href="/" class="header-logo" title="The Art of Machine Learning">The Art of Machine Learning</a>
  <ul class="header-links">

    <li>
        <a href="https://yinniyu.github.io/about">About Me
        </a>
      </li>
    
    <li>
        <a href="https://yinniyu.github.io/toolbox">My Toolbox</a>
     </li>
     
    
    <li>
        <a href="https://github.com/yinniyu" rel="noreferrer noopener" target="_blank" title="GitHub">
          <span class="icon icon-github"></span>
        </a>
      </li>
    
    
      <li>
        <a href="https://linkedin.com/in/jenny-yu-b495243" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <span class="icon icon-linkedin"></span>
        </a>
      </li>   
  </ul>
</nav>

        
        <article class="article appear">
          <header class="article-header">
            <h1>Looking deeper with LIME</h1>
             <figure>
              <img src='images/lime/localized_linear.png' alt="lime_local" >
               <figcaption>Source:Ribeiro et al. <i>Why should I trust you? Explaining the predictions of any classifier.</i></figcaption></figure>
            <p>Currently, there exists a big gap between AI applications and governance of these technology. Many AI 'black box' 
            are based on some flavors of
            neural network or deep learning building blocks, which makes model interpretability a challenging task. While the 
            stakes are low if a convolutional neural net misclassifies a Shar Pei for a bath towel, it's a different realm if AI is 
            performing automated medical diagnostics, making decisions on credit applications, or even finding <a href='https://en.wikipedia.org/wiki/Person_of_Interest_(TV_series)'>
              <u>'Person of Interest' </u></a>. The new EU general data protection 
            regulation (GDPR), effective May 25th, 2018, requires a "right to explanation" for human subjects of AI/automated system. 
            This means a subject/consumer has the right to obtain an explanation of the automated decision, and the right to opt out of 
            a decision based solely on AI/automated algorithm that produces legal effects on them (i.e, job recruiting,
            loan applications) without human intervention. Back in 2016, researchers (Ribeiro et al) from University of 
            Washington published an explanation technique called LIME that explains the predictions of any classifier in an 
            interpretable manner. The output of LIME can be used to assess trustworthiness of models by visualizing feature importance. In this post,
            I will apply LIME to evaluate the DenseNet melanoma model.
            
            <div class="article-list-footer">
              <span class="article-list-date">
                May 1st, 2018
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
              10 minute read
              </span>
              
              <span class="article-list-divider">-</span>
              <div class="article-list-tags">
                
                <span class="article-list-divider">Python, Image Classification, LIME, DeepLearning </span>
              </div>
            </div>
          </header>

          <div class="article-content">


<h2>LIME - a model of a model</h2>
  
<p> Local Interpretable Model-Agnostic Explanations (LIME) is an algorithm that explains a given prediction by approximating 
a separate interpretable model locally around the prediction. To quote the lead author Marco Ribeiro:
  
<blockquote class="w3-panel w3-leftbar w3-light-grey">
  " <b>Local </b>refers to local fidelity - i.e., we want the explanation to really reflect the behaviour of the classifier 
  'around' the instance being predicted. This explanation is useless unless it is <b>interpretable</b> - that is, 
  unless a human can make sense of it. 
  Lime is able to explain any model without needing to 'peek' into it, so it is <b>model-agnostic</b>."
</blockquote> 
  
  The picture at the top of this post illustrates fitting a localized linear model for a given prediction. For image classification, LIME
            will perturbe the original image to generate a set of data. For each perturbed sample, it calculates the 
            probability that the image belongs to a given class according to the model. Then it learns a locally faithful linear model,
            giving weights to perturbed data points by their proximity to the original image (Fig.1.).
        
<figure>
    <center><img src="images/melanoma/LIME_example.jpg" alt="frog and lime" ></center>
   <figcaption>Fig.1.Explaining feature importance of a frog using LIME. (source:https://www.kdnuggets.com/2016/08/introduction-local-interpretable-model-agnostic-explanations-lime.html)  </figcaption>
   </figure>
<h2>Data Processing </h2>
    <p>The dermoscopic images had many distracting artifacts (e.g, vignette borders, measurement scale, hair, etc) that can possilby interfere with the model learning. 
      Instead of writing a code for each type of artifacts to clean up the image,
      I realize it's more efficient to apply binary segmentation mask to select for the general lesion area. For segmentation, I used the SegNet autoencoder 
  model where it achieved a Dice coefficient (metric of overlap) of 78% on validation data.</p>

    <p>Another issue is that the malignant category was underpresented (20%), and it's difficult to  train a deep neural net 
      model with fewer than 1,000 pictures per class(yes, I did try). I used augmentation (rotation and translation)
      to produce copies of the original malignant images to mitigate the under-representation. No other augmentation methods were used at this stage. </p>   

 <h2>Transfer the learning with Bottleneck </h2>  
        <p>There's a wonderful <a href='https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html'>
           tutorial</a> by the creator of Keras illustrating transfer learning for deep neural net. What it means is that you take 
          advantage of fine-tuned model trained on large datasets, 
          and start training new data with the pre-trained weights to save time and computational cost. One common way to do that is 
          using 'bottleneck' features - output of the 'bottom' part of a deep learning model, essentially it's all the convolutional layers (everything up to the fully-connected layers ). I saved the bottleneck features as 
          numpy array, then train a customized 'top' part of the model for a binary classifier (see code below). Training the top part separately ensures that the 
          top model weight provides a consistent and smooth training when it's attached to the bottom model later on.</p>

<figure class="highlight"><pre><code class="language-Python" data-lang="Python">
base_model = MobileNet(input_shape=(192,192,3), include_top=False,weights='imagenet')

def Top_model(input_dim):
    top_model = Sequential()
    top_model.add(Flatten(input_shape=input_dim))
    top_model.add(Dense(512, activation='relu'))
    top_model.add(Dropout(0.6))
    top_model.add(Dense(512, activation='relu'))
    top_model.add(Dropout(0.6))
    top_model.add(Dense(1, activation='sigmoid'))
   
    return top_model

model2=Top_model(train_data.shape[1:])
model2.compile(optimizer=RMSprop(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy',f1])

model2.fit(train_data, train_labels,
              epochs=5, batch_size=32,verbose=1,
              validation_data=(validation_data, validation_labels))

################# Use functional API to  create the full model#############
top_model2 = Top_model(base_model.output_shape[1:])
top_model2.load_weights('bottleneck_fc_model(3layer)_512size_mobilenet.h5')
model = Model(input=base_model.input, output=top_model2(base_model.output))

for layer in model.layers[:4]:
    layer.trainable=False
model.compile(loss='binary_crossentropy',
              optimizer=SGD(lr=1e-3, momentum=0.9),
              metrics=['accuracy',f1])
</code></pre></figure>
<p>After training the top model, I augmented the image files with <code class='highlighter-rouge'>ImageDataGenerator()</code> along with 
  the default preprocessing function to scale the pixel values. I chose Stochastic Gradient Descend as the optimizer 
  because that was the optimizer used on the original MobileNet.</p>

<figure class="highlight"><pre><code class="language-Python" data-lang="Python">
train_datagen = ImageDataGenerator(
        height_shift_range=0.2,
        shear_range=0.4,
        rotation_range=40,
        preprocessing_function=preprocess_input)

validation_datagen=ImageDataGenerator(preprocessing_function=preprocess_input)

ROWS=192
COLS=192
train_generator = train_datagen.flow_from_directory(
            train_data_dir,
            classes=['benign_masked','malignant_masked'],
            target_size=(ROWS, COLS),
            batch_size=32,
            class_mode='binary',
            shuffle='True')

validation_generator = validation_datagen.flow_from_directory(
            validation_data_dir,
            classes=['benign_masked','malignant_masked'],
            target_size=(ROWS, COLS),
            batch_size=32,
            class_mode='binary',
            shuffle='True')

from sklearn.utils import class_weight
class_weight = class_weight.compute_class_weight('balanced', np.unique(train_labels), train_labels)

early_stopping =EarlyStopping(monitor='val_loss', patience=4)
model_checkpoint = ModelCheckpoint('training_weights_mobilenet_bottlneck.h5',
                                    monitor='val_loss', save_best_only=True,save_weights_only=True)

model_full=model.fit_generator(train_generator,
                           steps_per_epoch=86,
                           validation_data=validation_generator,
                           validation_steps=1,
                           class_weight=class_weight,
                           callbacks=[early_stopping, model_checkpoint],
                           epochs=20,verbose=1)  
</code></pre></figure>
  
        
  
        <p>Binary cross entropy was a loss function used to optimize the model(the lower the score the better), it takes into account <i>confidence</i> of the prediction along with 
          accuracy. The validation binary cross entropy was <b>0.43</b>, F1 score (balanced accuracy accounting for imbalanced 
          class distribution) was <b>86%</b>, and AUC for ROC was <b>0.89 </b>(1 being the best).</p>
  

  <figure>
    <center><img src="images/melanoma/roc.png" alt="ROC of validation" ></center>
   <figcaption>Fig.3. ROC of the validation dataset showing the trade-off between sensitivity (True Positive) and specificity (1-False Positive). 
     Area under the curve is the overall performance metric.
   </figcaption>
   </figure>
  
<p>While the AUC score was lower than the ones presented by Esteve et al[4], I should point out that this MobileNet model was trained on 
  a smaller training dataset provided by <a href='https://challenge.kitware.com/#challenge/583f126bcad3a51cc66c8d9a'> ISIC 2017 Challenge</a>. 
  I expect that better performance (>90% balanced accuracy) can be achieved with a larger training set, as well as bigger image array. For my own future reference, here are some things to 
  keep in mind when training a deep learning model for image classification:
<ul>
  <li>Pay attention to preprocessing step for a given deep neural net model. It's NOT always mean centered or standard deviation normalized to 1.
</li>
  <li>Sanity check: make sure the images in trainnig and test sets have more or less the same distribution.</li>
  <li>Explore augmentation options such as contrasting or adaptive histogram equilization. It is a handy tool especially
    when poor lighting or resolution of the digital image becomes an issue (Note to self: need to  explore this further).</li>
  <li>Obviously there's room for improvement. For example,the number of filters in each layer and dropout rate can be tuned.
</ul>  

<p><b>Note: For Fig.1, the center 4 images are melanoma, and the rest are benign. </b></p>       
          
<h3> REFERENCES </h3>
  <p>[1]Ribeiro et al. <b>Why should I trust you? Explaining the predictions of any classifier</b>, https://arxiv.org/pdf/1602.04938.pdf </p>  
<div id="disqus_thread" class="article-comments"></div>
<script src="https://chalk-1.disqus.com/embed.js" async defer></script>
<noscript>Please enable JavaScript to view the comments.</noscript>
          
</article>
<footer class="footer appear">
  <p>
    Chalk is a high quality, completely customizable, performant and 100% free
    blog template for Jekyll built by
    <a href="/about" title="About me">Nielsen Ramon</a>. Download it <a href="https://github.com/nielsenramon/chalk" rel="noreferrer noopener" target="_blank" title="Download Chalk">here</a>.
  </p>
</footer>

      </div>
    </div>
  </main>
  
  <script>
    window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
    ga('create','UA-28631876-6','auto');ga('send','pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async></script>


<script src="/assets/vendor.js"></script>



  <script src="/assets/webfonts.js"></script>




  <script src="/assets/scrollappear.js"></script>



<script src="/assets/application.js"></script>


</body>
</html>

    Â© 2018 GitHub, Inc.
    Terms
    Privacy
    Security
    Status
    Help

    Contact GitHub
    API
    Training
    Shop
    Blog
    About

