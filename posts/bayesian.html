<!doctype html>
<html lang="en">

<head>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The Art of Machine Learning</title>
  <meta name="description" content="Machine Learning, Deep Learning, Jenny Yu, denver ">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Bayesian methods">
  <meta property="og:type" content="website">
  <meta property="og:url" content="http://yinniyu.github.io/posts/bayesian.html">
  <meta property="og:site_name" content="The Art of Machine Learning">
  <meta property="og:image" content="images/bayesian/bayesian_joke(xkcd).png">

 
  <link rel="apple-touch-icon" href="/assets/apple-touch-icon.png">
  <link href="http://chalk.nielsenramon.com/feed.xml" type="application/rss+xml" rel="alternate" title="Chalk Last 10 blog posts" />

    <link rel="stylesheet" href="/assets/light.css">

  
</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav appear">
  <a href="/" class="header-logo" title="The Art of Machine Learning">The Art of Machine Learning</a>
  <ul class="header-links">

    <li>
        <a href="https://yinniyu.github.io/about">About Me
        </a>
      </li>
    
    <li>
        <a href="https://yinniyu.github.io/toolbox">My Toolbox</a>
      </li>
    
     <li>
        <a href="https://github.com/yinniyu" rel="noreferrer noopener" target="_blank" title="GitHub">
          <span class="icon icon-github"></span>
        </a>
      </li>
    
    
      <li>
        <a href="https://linkedin.com/in/jenny-yu-b495243" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <span class="icon icon-linkedin"></span>
        </a>
      </li>   
  </ul>
</nav>

        
        <article class="article appear">
          <header class="article-header">
            <h1>Bayesian approach and probabilistic programing</h1>
            <figure>
              <p align='center'><img src='images/bayesian/bayesian_joke(xkcd).png' alt="bayes" ></p>
               <figcaption>Source:XKCD comic.</figcaption></figure>
            <p>Most people working with statistics have heard of Bayes Theorem, but in doing probilistic programming there's a lot of 
              nuances in implementing Bayes theorem. In this post I'll briefly summarize the Bayesian approach and show an example of working with mixed type
              of data (continuous and categorical) using Python's PYMC3 package (yes there will be no coin example). While there're many tutorials out there, it was not straightforward 
            to figure out how to deal with categorical/dummy variables in PYMC3.</p>
            
            <div class="article-list-footer">
              <span class="article-list-date">
                , 2018
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
              15 minute read
              </span>
              
              <span class="article-list-divider">-</span>
              <div class="article-list-tags">
                
                <span class="article-list-divider">Python, probabilistic programming, PYMC3</span>
              </div>
            </div>
          </header>

          <div class="article-content">
  <p>To better understand Bayesian statistics I recommend this great 'laymen' <a href='https://www.nytimes.com/2014/09/30/science/the-odds-continually-updated.html'> 
    article </a>published in New York Times. The gist can be 
            captured by this quote <i> "The essence of the frequentist technique is to apply probability to data. If you suspect your friend has a weighted coin, 
    for example, and you observe that it came up heads nine times out of 10, a frequentist would calculate the probability of 
    getting such a result with an unweighted coin [null hypothesis]. The answer (about 1 percent) is not a direct measure of the 
    probability that the coin is weighted; it’s a measure of how improbable the nine-in-10 result is — a piece of information that can be useful in investigating your suspicion. 
    By contrast, <b>Bayesian calculations go straight for the probability of the hypothesis</b>, factoring in not just the data from the coin-toss experiment but any other relevant information —
    including whether you have previously seen your friend use a weighted coin." </i> In a mathematical sense, frequentist does repeated 
    sampling to estimate an underlying parameter, and treats that parameter as fixed. In the Bayesian world, you can parameterize 
    prior belief and the underlying distribution of observed data, and that belief 
    can be <a href="https://www.analyticsvidhya.com/wp-content/uploads/2016/06/12-768x475.jpg">
      updated</a> with each new observation (posterior probability), and be described as a degree of belief. 
    In this case, it's the data that is fixed (i.e., you're concerned with the probability 
    of the outcome of one event).</p>


<h2 id="pymc3">Probabilistic Programming </h2>

<p>Bayesian theory is the foundation for Probabilistic Programming, which 'learns' the parameters of interest using sophisticated sampling techniques (as 
            oppose to solving linear equation using OLS), and returns a probability distribution of the parameters. To give a concrete 
            example, suppose you have a linear equation y=w1*x+w0 and the goal is to find the weights (w0, w1). In order to estimate these
            parameters, you will need a sampling algorithm to search for the parameter value within a distribution 
              that's updated in every iteration.
  One of the most common algorithms used is Markov-Chain-Monte-Carlo (MCMC) sampling.  The general steps are outlined below [1]:</p>
            <li>set up prior distributions for weights w0, w1. </li>
            <li>sample from the probability distribution to get a vector of values for the weights (<b>w</b>).</li>
            <li>Generate prediction values(y^), and calculate the loss term using the negative log of the likelihood distribution.</li>
            <li>Update the weights distribution through minimization of the loss function.</li>
            <li>Repeat the sampling process to get better estimation of the weights by proposing 
              new values of vector <b>w</b> close to the previous values. The MCMC method moves along the joint distribution according to some pre-defined threshold values.</li>
  <br>            
   <p>The MCMC method is very computationally expensive. To make it more efficient, an improvement was made in the form of 
     Hamiltonian Monte Carlo method(HMC), which takes advantage of the gradient of the log posterior-density to guide the sampling toward 
     region of higher likelihood.  No-U-Turn Sampler (NUTS) was 
     a more recent improvement on top of HMC, it offers adaptive tuning of hyperparameter values for determining number of iterations/jumps
       to make.</p>
    <p>To better understand how loss term is related to the log likelihood, I found a ppt lecture given by Dr. Piyush Rai for 
              CS5350/6350 at the University of Utah. As shown in Fig.1., the takeaway is that log of posterior distribution (P(<b>w</b>|D)) is maximized by looking for the 
              optimal <b>w</b>, and argmax(log P(<b>w</b>|D) is equivalent to  argmin of (-log P(D|<b>w</b>)-log(P(<b>w</b>)). This set up
              showes that negative log of the likelihood function is the loss term, and negative log of <b>w</b> serves as regularization term.
              With auto differentiation, the gradient of the -log P(D|<b>w</b>) can be found to faciliate more efficient sampling for <b>w</b>.

       <figure>
      <center><img src="images/bayesian/log_likelihood.png" alt="log_likelihood" ></center>
   <figcaption>Fig.1. Finding Maximum-a-Posteriori solution for <b>w</b>. 
   </figcaption>
   </figure>            

<h2 id="pymc3">PyMC3</h2>

<p>As stated in the PyMC3 tutorial, NUTS is best used for continuous parameter sampling, and Metropolis for categroical parameter. 
            In the figure below, I share an example of running a logistic model consists of both continuous and categorical inputs to 
            produce a binary outcome value. I find that the easiest way to incorporate mixed data type in PyMC3 is to define your own 
            function (instead of using the default pm.glm.GLM() function).</p>
     <figure>
      <center><img src="images/bayesian/pymc3_lr.png" alt="pymc3_lr" ></center>
   <figcaption>Fig.2. Example of training a probilistic programming model using PyMC3 sampling, and generating the sampling results (trace plot).
   </figcaption>
   </figure>
   
  
<p>In defining the distribution of the categorical variables (there were 7), either Bernoulli or Binomial distributions can be used.
  With <code class="highlighter-rouge">pm.Binomial()</code>, user must enter number of trials (e.g., n=10) since Binomial function 
            is concerned with number of trials. Another useful feature of PyMC3 is that user can define multiple steps to assign 
            different sampling method to the desired variable(s). It's also helpful to use the <code class="highlighter-rouge">
  share()</code> function from PyMC3 to later share the out-of-sample test data in order to run inference on the trained modl.</p>
  


  <h3> Conclusion </h3>
  <p> After looking at the results from the three methods, it looks like the performances of KNN and RF are very sensitive to 
    % of missing values. This is not surprising because 
    these non-parametric algorithms impute based on finding its neighbors - and the more data observed the better the neighborhood can be mapped. 
    Gower's matrix provided better imputation results for numerical data compared to RF, due to the way dissimilarity distance is calculated. However, RF performed better 
    in categorical imputation. One way to approach the Titanic dataset is to use RF imputation on categorical variables and use KNN on numerical variables.
    However, it should be noted that RF imputation is computationally expensive. If computation cost is not a big concern, 
    one way to account for uncertainty is to have multiple imputed datasets for model fitting, where the model outputs are averaged for prediction.
   
  
  <h3> REFERENCES </h3>
  <p>[1]Sedar, J.,<i> Bayesian Inference with PyMC3 - Part 1</i>. https://blog.applied.ai/bayesian-inference-with-pymc3-part-1/</p>
  <p>[2]Hoffman, M.D., Gelman, A. (2011). <b>The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian 
    Monte Carlo </b>.</p>

  
    
    
    
            <div id="disqus_thread" class="article-comments"></div>
            <script src="https://chalk-1.disqus.com/embed.js" async defer></script>
            <noscript>Please enable JavaScript to view the comments.</noscript>
          
        </article>
        <footer class="footer appear">
          
   
    Chalk is a high quality, completely customizable, performant and 100% free
    blog template for Jekyll built by
    <a href="/about" title="About me">Nielsen Ramon</a>. Download it <a href="https://github.com/nielsenramon/chalk" rel="noreferrer noopener" target="_blank" title="Download Chalk">here</a>.
  </p>
</footer>

      </div>
    </div>
  </main>
  
  <script>
    window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
    ga('create','UA-28631876-6','auto');ga('send','pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async></script>


<script src="/assets/vendor.js"></script>



  <script src="/assets/webfonts.js"></script>




  <script src="/assets/scrollappear.js"></script>



<script src="/assets/application.js"></script>


</body>
</html>
